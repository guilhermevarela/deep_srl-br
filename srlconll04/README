========================================================================

8th Conference on Computational Natural Language Learning (CoNLL-2004)


Shared Task Distribution -- Official Release -- March 1, 2004

========================================================================

Xavier Carreras and Lluís Màrquez

conll04st@lsi.upc.es
http://www.lsi.upc.es/~conll04st

========================================================================


DISCLAIMER

Participants are required to have a valid licence of the Penn Treebank
II in order to use the data in this distribution.  Please, be in touch
with the organizers if you would like to participate and do not have
the license.



GENERAL

This is the 20040518 release of the data for the CoNLL-2004 shared
task. This is a post-conference release, and test material is now 
included in the data. 
 
The shared task of CoNLL-2004 concerns the recognition of semantic
roles, for the English language. Given a sentence, the task consists of
analyzing the propositions expressed by some target verbs of the
sentence. In particular, for each target verb all the constituents in
the sentence which fill a semantic role of the verb have to be
recognized. 



GOAL

The data is a collection of sentences, each of which contains a number
of target verbs.  The goal of the task is to develop a machine
learning system to recognize participants of the propositions governed
by the target verbs. For simplicity, we will refer as arguments to all
kinds of participants in a proposition, including adjunctives,
references and the verb realization. The output, thus, is a set of
arguments for each proposition.

Following earlier CoNLL Shared Tasks, the input information for each
sentence consists of: words, part-of-speech tags, base chunks, clauses
and named entities.

Training and development data are provided to build the learning
system. The training dataset will contain both correct and predicted
input, as well as the correct output, and will be used for training
systems. The development data will contain predicted input and the
correct output, and will be used to tune parameters of the learning
systems.

Evaluation will be performed on a separate test set, which will be
provided only with predicted input data. A system will be evaluated
with respect to precision, recall and the F1 measure of recognized
arguments. For an argument to be correctly recognized, both the words
spanning the argument and its type have to be correct. The srl-eval.pl
program, distributed by the organization, is the official program to
compute the scores.

The Shared Task evaluation is separated into two challenges:

*/ Closed Challenge: Systems have to be developed strictly with the data
    provided, which consists of input/output data and the official
    external resources. Since we are providing the correct annotations
    for the input data, a system is allowed either to be trained to
    predict the input part, or to make use of an external tool
    strictly developed within this setting (such as previous CoNLL
    shared task systems). The aim of this challenge is to produce a
    ranking of systems with respect to their F1 measure, and to
    compare their performance in a fair environment.


*/ Open Challenge: Systems can be developed making use of any kind of
    external tools and resources. The only condition is that such
    tools or resources have not been developed with the annotations of
    the test set (WSJ section 21), both for the input and output parts
    of the data. The aim of this challenge is to contrast the
    performance of a system in the closed challenge with respect to
    the same system while using available tools or resources. Thus, we
    are interested in learning methods which take advantage of useful
    tools or resources and improve substantially the performance.  The
    comparison of different systems here may not be fair, and thus
    ranking of systems is not particularly interesting.



DATA

The data consist of the sections of the Wall Street Journal part of
the Penn Treebank used in past editions of the CoNLL shared
tasks: training set (sections 15-18), development set (section 20) and 
test set (section 21). 

Annotations have been derived from the Penn TreeBank II project [TB] for
the syntactic information, and from the PropBank project [PB] for the
propositional analysis (February 2004 PropBank release). 

Predicted information has been computed with state-of-the-art systems:
PoS tags with (Giménez and Màrquez 2003); base chunks and clauses with
(Carreras and Màrquez 2003); and named entities with (Chitieu and Ng
2003).



FORMAT

Here is an example of a fully-annotated sentence: 


   The         DT    B-NP  (S*    O       -        (A0*    (A0*       
   $           $     I-NP    *    O       -           *       *       
   1.4         CD    I-NP    *    O       -           *       *       
   billion     CD    I-NP    *    O       -           *       *       
   robot       NN    I-NP    *    O       -           *       *       
   spacecraft  NN    I-NP    *    O       -           *A0)    *A0)    
   faces       VBZ   B-VP    *    O       face      (V*V)     *       
   a           DT    B-NP    *    O       -        (A1*       *       
   six-year    JJ    I-NP    *    O       -           *       *       
   journey     NN    I-NP    *    O       -           *       *       
   to          TO    B-VP  (S*    O       -           *       *       
   explore     VB    I-VP    *    O       explore     *     (V*V)     
   Jupiter     NNP   B-NP    *    B-LOC   -           *    (A1*       
   and         CC    O       *    O       -           *       *       
   its         PRP$  B-NP    *    O       -           *       *       
   16          CD    I-NP    *    O       -           *       *       
   known       JJ    I-NP    *    O       -           *       *       
   moons       NNS   I-NP    *S)  O       -           *A1)    *A1)    
   .           .     O       *S)  O       -           *       *    


Annotations of a sentence are given using a flat representation in
columns, separated by spaces. Each column encodes an annotation with a
tagging along words.

For each sentence, the following columns are provided: 

    1  Words. 
    2  Part of Speech tags. 
    3  Chunks in IOB2 format. 
    4  Clauses in Start-End format. 
    5  Named Entities in IOB2 format. 

    6  Target verbs, marking N predicative verbs. This column, provided
       as input, specifies the governing verbs of the propositions to
       be analyzed. Each target verb is in its infinitive form. In
       some sentences, this column does not mark any verb (ie, N may
       be 0).
	
    7+ For each of the N target verbs, a column in Start-End format
       specifying the arguments of the proposition. These columns are
       the output of a system, that is, the ones to be predicted.
       These columns are not available for the test set. 


The IOB2 format represents chunks which do not overlap nor embed.
Words outside a chunk receive the tag O. For words inside a chunk of
type $k, the first word receives the "B-$k" tag (Begin), and the
remaining words receive the tag "I-$k" (Inside).
	     
The Start-End format represents phrases (clauses or arguments) which
do not overlap but may be embedded ones inside others. Each tag
represents which phrases start and end in a word, and is of the form
STARTS*ENDS. The STARTS part is a concatenation of "($k" parentheses,
each representing that a phrase of type $k starts in that word. The
ENDS part is a concatenation of "$k)" parentheses, each representing
that a phrase of type $k ends in that word. Finally, the concatenation
of all tags constitutes a well-formed bracketing. For example, the "*"
tag represents a word with no starts and ends; the "(A0*A0)" tag
represents a word constituting an A0 argument; and the "(S(S*S)" tag
represents a word which constitutes a base clause (labeled S) and
starts another higher-level clause.



NOTES ON THE STRUCTURE OF ARGUMENTS IN A PROPOSITION

Although we represent the arguments of each proposition in a format
which allows embedding, no embedding is observed in arguments of a
proposition governed by a verb. 

Some arguments of a proposition can appear in a sentence split into
many discontiguous phrases. In this case, each phrase of an argument
of type $k is represented as a phrase in Start-End format: the first
phrase appears with label $k, and the remaining phrases appear with
label "C-$k" (Continuation). For a system to correctly recognize a
discontinuous argument, all and only its phrases have to be
correctly recognized.



FILES 

Each dataset is distributed split into four separate files:

    * words : Sentences and words.
    * synt  : Syntactic information, including PoS tags, chunks and clauses.
    * ne    : Named entities.
    * props : Target verbs and proposition arguments

Use the standard UNIX command paste to merge the files into a single
file. The following example command generates a training set with
complete predicted input information:

  \> paste -d ' ' words.train synt.train.pred ne.train.pred props.train > train


The following files are distributed in this release:

    README                   This file.

    words.train.gz           Training sentences.
    words.dev.gz             Development sentences. 
    words.test.gz            Test sentences. 

    synt.train.gold.gz       Correct syntactic annotations for training.
    synt.dev.gold.gz         Correct syntactic annotations for development.

    synt.train.pred.gz       Predicted syntactic annotations for training.
    synt.dev.pred.gz         Predicted syntactic annotations for development.
    synt.test.pred.gz        Predicted syntactic annotations for test.
	
    ne.train.pred.gz         Predicted named entities for training. 
    ne.dev.pred.gz           Predicted named entities for development. 
    ne.test.pred.gz          Predicted named entities for test. 

    props.train.gz           Correct props file for training. 
    props.dev.gz             Correct props file for development.

    targets.test.gz          A column of target verbs for the test. 
    props.test.gz            Correct props file for the test. 
  
    srl-eval.pl              Official script for evaluation.
    baseline.pl              A simple baseline system for the task.
    pb-frames.tar.gz         Scheme of rolesets for English verbs.
    senses.train.gz	     Manually annotated verb senses: training set.
    senses.dev.gz            Manually annotated verb senses: development set.
    senses.test.gz           Manually annotated verb senses: test set.
    

SOFTWARE

The srl-eval.pl program is the official script for evaluation of
CoNLL-2004 Shared Task systems. It expects two parameters: The first
is the name of the file containing correct propositions; the second is
the name of the file containing predicted propositions. Both files are
expected to follow the format of "props" files (first column: target
verbs; remaining columns: args of each target verb). It is required
that both files contain the same sentences and the same target verbs.
The program outputs performance measures based on precision, recall
and F1. The overall F1 measure will be the measure used to compare the
performance of systems. Use the option "-latex" to produce a table of
results in LaTeX.

The program is implemented in Perl, object oriented, with each object
embedded in the script. If you find bugs, please get in touch asap
with the organizers.


BASELINE 

Erik Tjonk Kim Sang, earlier organizer of Shared Tasks, has kindly
provided a baseline system for the task.  The script, implemented in
Perl, performs the following basic actions:

1. Tag main verb and successive particles as V phrases
2. Tag "not" and "n't" in main verb chunk as AM-NEG
3. Tag modal verbs in main verb chunk as AM-MOD
4. Tag first NP before main verb as A0
5. Tag first NP after main verb as A1
6. Tag "that", "which" and "who" before main verb as R-A0
7. Switch A0 and A1, and R-A0 and R-A1 if the main verb is part of a
   passive VP chunk (=contains form of "to be" without verb ending on "ing")


The script should be used as:

$  paste -d' ' words.dev synt.dev.pred props.dev | baseline.pl > file
$  srl-eval.pl props.dev file


It can be used the same way in the training and test sets (providing
only the column of targets for the test set). The overall evaluation
results (excluding V arguments) are the following:

                        corr.  excess  missed    prec.    rec.      F1
        --------------------------------------------------------------
            Training    15261   14092   34921    51.99   30.41   38.38
         Development     3370    3286    7751    50.63   30.30   37.91
                Test     3013    2505    6585    54.60   31.39   39.87
        --------------------------------------------------------------

Feel free to modify the code, and let us know any improvements of the
baseline performance.


OFFICIAL RESOURCES

[1] PropBank Frames: 

Scheme of rolesets for English verbs. A roleset, defined for a verb
acting in one of its verb senses, specifies the arguments that a verb
accepts, and defines which semantic role each argument is playing with
respect to the verb.  
(last updated: February 12, 2004)

[2] Verb Senses:
 
Manually annotated verb senses. The format of these files is just a
column which aligns with the column specifying target verbs of the
data. For each target verb, either the verb sense or a
non-disambiguated "XX" tag appears. Note that this information will
not be provided for the final test set.  
(last updated: May 17, 2004)



REFERENCES

(Carreras and Màrquez 2003)
       Xavier Carreras and Lluís Màrquez.
       "Phrase Recognition by Filtering and Ranking with Perceptrons" 
       In Proceedings of RANLP-2003, Borovets, Bulgaria, 2003.

(Chieu and Ng 2003) 
       Hai Leong Chieu and Hwee Tou Ng.
       "Named Entity Recognition with a Maximum Entropy Approach"
       In Proceedings of CoNLL-2003, Edmonton, Canada, 2003. 

(Giménez and Màrquez 2003)
       Jesús Giménez and Lluís Màrquez.
       "Fast and Accurate Part-of-Speech Tagging: The SVM Approach Revisited"
       In Proceedings of RANLP-2003, Borovets, Bulgaria, 2003.


[PB] PropBank Project:  http://www.cis.upenn.edu/~ace

[TB] Penn TreeBank II Project :  http://www.cis.upenn.edu/~treebank



---
