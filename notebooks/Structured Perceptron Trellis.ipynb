{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../models/')\n",
    "sys.path.insert(0,'../datasets/')\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from utils.info import get_db_bounds\n",
    "from datasets import propbankbr_arg2se, propbankbr_iob2arg\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tqdm\n",
    "from models import PropbankEncoder\n",
    "import config \n",
    "\n",
    "INPUT_DIR = '../datasets/binaries/1.0/'\n",
    "PROPBANK_WAN50_PATH = '{:}wan50/deep_wan50.pickle'.format(INPUT_DIR)\n",
    "PEARL_SRLEVAL_PATH = '../srlconll04/srl-eval.pl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPN Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma mente insana realiza um experimento *lúcido* com o dataset de chunking da conll e seu script de avaliação :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>P</th>\n",
       "      <th>FORM</th>\n",
       "      <th>GPOS</th>\n",
       "      <th>MARKER</th>\n",
       "      <th>ARG</th>\n",
       "      <th>T</th>\n",
       "      <th>IOB</th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>CHUNK_START</th>\n",
       "      <th>CHUNK_FINISH</th>\n",
       "      <th>CHUNK_LEN</th>\n",
       "      <th>CHUNK_CANDIDATE_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDEX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Brasília</td>\n",
       "      <td>PROP</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pesquisa_Datafolha</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>(A0*</td>\n",
       "      <td>A0</td>\n",
       "      <td>B-A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>publicada</td>\n",
       "      <td>V-PCP</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>A0</td>\n",
       "      <td>I-A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>hoje</td>\n",
       "      <td>ADV</td>\n",
       "      <td>0</td>\n",
       "      <td>*)</td>\n",
       "      <td>A0</td>\n",
       "      <td>I-A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>revela</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>1</td>\n",
       "      <td>(V*)</td>\n",
       "      <td>V</td>\n",
       "      <td>B-V</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>um</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>(A1*</td>\n",
       "      <td>A1</td>\n",
       "      <td>B-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>dado</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>supreendente</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>recusando</td>\n",
       "      <td>V-GER</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>uma</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>postura</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>radical</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>esmagadora</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>maioria</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>(</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>%</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>os</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>eleitores</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>quer</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>PT</td>\n",
       "      <td>PROP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>participando</td>\n",
       "      <td>V-GER</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Governo</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>Fernando_Henrique_Cardoso</td>\n",
       "      <td>PROP</td>\n",
       "      <td>1</td>\n",
       "      <td>*)</td>\n",
       "      <td>A1</td>\n",
       "      <td>I-A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  P                       FORM   GPOS  MARKER   ARG   T   IOB  \\\n",
       "INDEX                                                                    \n",
       "0       1  1                   Brasília   PROP       0     *   *     O   \n",
       "1       2  1         Pesquisa_Datafolha      N       0  (A0*  A0  B-A0   \n",
       "2       3  1                  publicada  V-PCP       0     *  A0  I-A0   \n",
       "3       4  1                       hoje    ADV       0    *)  A0  I-A0   \n",
       "4       5  1                     revela  V-FIN       1  (V*)   V   B-V   \n",
       "5       6  1                         um    ART       1  (A1*  A1  B-A1   \n",
       "6       7  1                       dado      N       1     *  A1  I-A1   \n",
       "7       8  1               supreendente    ADJ       1     *  A1  I-A1   \n",
       "8       9  1                          :     PU       1     *  A1  I-A1   \n",
       "9      10  1                  recusando  V-GER       1     *  A1  I-A1   \n",
       "10     11  1                        uma    ART       1     *  A1  I-A1   \n",
       "11     12  1                    postura      N       1     *  A1  I-A1   \n",
       "12     13  1                    radical    ADJ       1     *  A1  I-A1   \n",
       "13     14  1                          ,     PU       1     *  A1  I-A1   \n",
       "14     15  1                          a    ART       1     *  A1  I-A1   \n",
       "15     16  1                 esmagadora    ADJ       1     *  A1  I-A1   \n",
       "16     17  1                    maioria      N       1     *  A1  I-A1   \n",
       "17     18  1                          (     PU       1     *  A1  I-A1   \n",
       "18     19  1                         77    NUM       1     *  A1  I-A1   \n",
       "19     20  1                          %      N       1     *  A1  I-A1   \n",
       "20     21  1                          )     PU       1     *  A1  I-A1   \n",
       "21     22  1                         de    PRP       1     *  A1  I-A1   \n",
       "22     23  1                         os    ART       1     *  A1  I-A1   \n",
       "23     24  1                  eleitores      N       1     *  A1  I-A1   \n",
       "24     25  1                       quer  V-FIN       1     *  A1  I-A1   \n",
       "25     26  1                          o    ART       1     *  A1  I-A1   \n",
       "26     27  1                         PT   PROP       1     *  A1  I-A1   \n",
       "27     28  1               participando  V-GER       1     *  A1  I-A1   \n",
       "28     29  1                         de    PRP       1     *  A1  I-A1   \n",
       "29     30  1                          o    ART       1     *  A1  I-A1   \n",
       "30     31  1                    Governo      N       1     *  A1  I-A1   \n",
       "31     32  1  Fernando_Henrique_Cardoso   PROP       1    *)  A1  I-A1   \n",
       "32     33  1                          .     PU       1     *   *     O   \n",
       "\n",
       "       CHUNK_ID  CHUNK_START  CHUNK_FINISH  CHUNK_LEN  CHUNK_CANDIDATE_ID  \n",
       "INDEX                                                                      \n",
       "0             1            0             1          1                   0  \n",
       "1             2            1             4          3                  35  \n",
       "2             2            1             4          3                  35  \n",
       "3             2            1             4          3                  35  \n",
       "4             3            4             5          1                 126  \n",
       "5             4            5            32         27                 181  \n",
       "6             4            5            32         27                 181  \n",
       "7             4            5            32         27                 181  \n",
       "8             4            5            32         27                 181  \n",
       "9             4            5            32         27                 181  \n",
       "10            4            5            32         27                 181  \n",
       "11            4            5            32         27                 181  \n",
       "12            4            5            32         27                 181  \n",
       "13            4            5            32         27                 181  \n",
       "14            4            5            32         27                 181  \n",
       "15            4            5            32         27                 181  \n",
       "16            4            5            32         27                 181  \n",
       "17            4            5            32         27                 181  \n",
       "18            4            5            32         27                 181  \n",
       "19            4            5            32         27                 181  \n",
       "20            4            5            32         27                 181  \n",
       "21            4            5            32         27                 181  \n",
       "22            4            5            32         27                 181  \n",
       "23            4            5            32         27                 181  \n",
       "24            4            5            32         27                 181  \n",
       "25            4            5            32         27                 181  \n",
       "26            4            5            32         27                 181  \n",
       "27            4            5            32         27                 181  \n",
       "28            4            5            32         27                 181  \n",
       "29            4            5            32         27                 181  \n",
       "30            4            5            32         27                 181  \n",
       "31            4            5            32         27                 181  \n",
       "32            5           32            33          1                 560  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfgs = pd.read_csv('../datasets/csvs/1.0/gs.csv', index_col=0, sep=',', encoding='utf-8')\n",
    "column_files = [\n",
    "    '../datasets/csvs/1.0/column_chunks/chunks.csv',\n",
    "    '../datasets/csvs/1.0/column_predmarker/predicate_marker.csv',\n",
    "    '../datasets/csvs/1.0/column_shifts_ctx_p/form.csv',\n",
    "    '../datasets/csvs/1.0/column_shifts_ctx_p/gpos.csv',\n",
    "    '../datasets/csvs/1.0/column_shifts_ctx_p/lemma.csv',\n",
    "    '../datasets/csvs/1.0/column_iob/iob.csv',\n",
    "    '../datasets/csvs/1.0/column_t/t.csv'\n",
    "]\n",
    "\n",
    "for col_f in column_files:\n",
    "    _df = pd.read_csv(col_f, index_col=0, encoding='utf-8')\n",
    "    dfgs = pd.concat((dfgs, _df), axis=1)\n",
    "\n",
    "DISPLAY_COLUMNS = ['ID', 'P', 'FORM', 'GPOS', 'MARKER', 'ARG', 'T', 'IOB',\n",
    "                   'CHUNK_ID', 'CHUNK_START', 'CHUNK_FINISH', 'CHUNK_LEN', 'CHUNK_CANDIDATE_ID']            \n",
    "dfgs[DISPLAY_COLUMNS].head(33)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Encodings\n",
    "\n",
    "Propbank Encoder holds an indexed version of propbank dataset an answers to FOUR different dataformats: \n",
    "* CAT: this is the raw categorical data.\n",
    "* EMB: tokens are embedding using GloVe embeddings.\n",
    "* HOT: onehot encoding of the words and tokens.\n",
    "* IDX: dense indexed representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ENCODER\n",
    "propbank_encoder = PropbankEncoder.recover(PROPBANK_WAN50_PATH)\n",
    "db = propbank_encoder.db\n",
    "lex2idx = propbank_encoder.lex2idx\n",
    "idx2lex = propbank_encoder.idx2lex\n",
    "\n",
    "# FOR TEXTUAL DATA ONLY\n",
    "tok2idx = propbank_encoder.tok2idx\n",
    "lex2tok = propbank_encoder.lex2tok\n",
    "idx2word = propbank_encoder.idx2word\n",
    "\n",
    "#Numpyfy embeddings\n",
    "embeddings = propbank_encoder.embeddings\n",
    "\n",
    "embeddings = np.concatenate([np.array(embs).reshape((1,50))\n",
    "                             for embs in embeddings], axis=0)\n",
    "n_targets = len(lex2idx['IOB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall:\n",
      "  \tattributes:43\trecords:138378\tvocab:13206\tpropositions:5776\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "'''Overall:\n",
    "  \\tattributes:{:}\\trecords:{:}\\tvocab:{:}\\tpropositions:{:}'''\n",
    "    .format(len(db), len(db['ARG'].keys()), \n",
    "            len(set([form for _, form in db['FORM'].items()])),\n",
    "            len(set([p for _, p in db['P'].items()]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Wrangling\n",
    "\n",
    "## 2.1 Helpful  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_type(ds_type, db):\n",
    "    '''Filters only records from train dataset\n",
    "    '''\n",
    "    lb, ub = get_db_bounds(ds_type)\n",
    "\n",
    "    sel_keys_ = {key_ for key_, prop_ in db['P'].items() if prop_ >= lb and prop_ < ub}\n",
    "\n",
    "    return {\n",
    "                attr_:{ idx_: i_\n",
    "                        for idx_, i_ in dict_.items() if idx_ in sel_keys_\n",
    "                      }        \n",
    "                for attr_, dict_  in db.items()\n",
    "            }\n",
    "\n",
    "def make_propositions_dict(db):\n",
    "    '''Reindex db by propositions creating a nested dict in which the\n",
    "        outer key is the proposition        \n",
    "    '''\n",
    "    \n",
    "    triple_list = []\n",
    "    prev_idx = min(db['P'].keys())\n",
    "    prev_prop = min(db['P'].values()) - 1 # Always enter first time\n",
    "    first = True\n",
    "    for idx, prop in db['P'].items():        \n",
    "        if prev_prop != prop:\n",
    "            if not first:\n",
    "                ub = prev_idx\n",
    "                triple_list.append((lb, ub, prev_prop))\n",
    "            lb = idx\n",
    "            first = False\n",
    "        prev_prop = prop\n",
    "        prev_idx = idx\n",
    "    triple_list.append((lb, prev_idx, prev_prop))\n",
    "            \n",
    "\n",
    "        \n",
    "    prop_set = set(db['P'].values())\n",
    "    return { prop_:\n",
    "                    {\n",
    "                        attr_:{ idx_: dict_[idx_]\n",
    "                                for idx_ in range(lb_, ub_ + 1, 1)\n",
    "                          }        \n",
    "                        for attr_, dict_ in db.items() if attr_ not in ('P',)\n",
    "                    }\n",
    "             for lb_, ub_, prop_ in  triple_list\n",
    "            }, {prop_: ub_ - lb_ + 1 for lb_, ub_, prop_ in  triple_list}   \n",
    "\n",
    "\n",
    "def numpfy_propositions_dict(prop_dict, proplen_dict):\n",
    "    '''Converts inner dict examples into numpy arrays\n",
    "    '''\n",
    "    prop_dict_ = defaultdict(dict)    \n",
    "    for prop, columns_dict in prop_dict.items():\n",
    "        len_ = proplen_dict[prop]\n",
    "        shape_ = (len_, 1)\n",
    "        for column, values_dict in columns_dict.items():\n",
    "            tuple_list = [idx_value \n",
    "                          for idx_value in values_dict.items()]\n",
    "\n",
    "            tuple_list = sorted(tuple_list, key=lambda x: x[0])            \n",
    "            # Converts lexicon (raw/indexed) into token (embedded/indexed)\n",
    "            if (('FORM' in column) or ('LEMMA' in column)):\n",
    "                values_list = [tok2idx[lex2tok[idx2word[tuple_[1]]]]                \n",
    "                                   for tuple_ in tuple_list]\n",
    "            else:\n",
    "                values_list = [tuple_[1] for tuple_ in tuple_list]\n",
    "\n",
    "            prop_dict_[prop][column]  = np.array(values_list).reshape(shape_)\n",
    "\n",
    "    return prop_dict_        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbtrain = filter_type('train', db)\n",
    "dbvalid = filter_type('valid', db)\n",
    "dbtest = filter_type('test', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "  \tattributes:43\trecords:0128258\tvocab:0012469\tpropositions:5295\n",
      "Valid:\n",
      "  \tattributes:43\trecords:0004893\tvocab:0001180\tpropositions:239\n",
      "Test:\n",
      "  \tattributes:43\trecords:0005123\tvocab:0001139\tpropositions:239\n",
      "Overall:\n",
      "  \tattributes:129\trecords:0138274\tvocab:0013196\tpropositions:5773\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "'''Train:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbtrain), \n",
    "            len(dbtrain['ARG'].keys()), \n",
    "            len(set([form for _, form in dbtrain['FORM'].items()])),\n",
    "            len(set([p for _, p in dbtrain['P'].items()]))))\n",
    "\n",
    "print(\n",
    "'''Valid:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbvalid),\n",
    "            len(dbvalid['ARG'].keys()), \n",
    "            len(set([form for _, form in dbvalid['FORM'].items()])),\n",
    "            len(set([p for _, p in dbvalid['P'].items()]))))\n",
    "\n",
    "print(\n",
    "'''Test:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbtest),\n",
    "            len(dbtest['ARG'].keys()), \n",
    "            len(set([form for _, form in dbtest['FORM'].items()])),\n",
    "            len(set([p for _, p in dbtest['P'].items()]))))\n",
    "\n",
    "print(\n",
    "'''Overall:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbtrain) + len(dbvalid) + len(dbtest), \n",
    "            len(set(list(dbtrain['ARG'].keys()) + list(dbvalid['ARG'].keys()) + list(dbtest['ARG'].keys()))), \n",
    "            len(set([form for _, form in dbtrain['FORM'].items()] +\n",
    "                    [form for _, form in dbvalid['FORM'].items()] +\n",
    "                    [form for _, form in dbtest['FORM'].items()])),\n",
    "             len(set([form for _, form in dbtrain['P'].items()] +\n",
    "                    [form for _, form in dbvalid['P'].items()] +\n",
    "                    [form for _, form in dbtest['P'].items()]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Nested proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbtrain, lentrain = make_propositions_dict(dbtrain)\n",
    "dbvalid, lenvalid = make_propositions_dict(dbvalid)\n",
    "dbtest, lentest = make_propositions_dict(dbtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "  \tattributes:43\trecords:0128258\tvocab:0012469\tpropositions:5295\n",
      "Valid:\n",
      "  \tattributes:43\trecords:0004893\tvocab:0001180\tpropositions:239\n",
      "Test:\n",
      "  \tattributes:43\trecords:0005123\tvocab:0001139\tpropositions:239\n",
      "Overall:\n",
      "  \tattributes:43\trecords:0138274\tvocab:0013196\tpropositions:5773\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "'''Train:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbtrain[1]) + 1, \n",
    "            sum([len(d['ARG']) for p, d in dbtrain.items()]), \n",
    "            len(set([v                    \n",
    "                     for p, d in dbtrain.items()\n",
    "                     for v in d['FORM'].values()])),\n",
    "            len(lentrain)))\n",
    "\n",
    "print(\n",
    "'''Valid:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbvalid[min(dbvalid)]) + 1,\n",
    "            sum([len(d['ARG']) for p, d in dbvalid.items()]), \n",
    "            len(set([v\n",
    "                     for p, d in dbvalid.items()\n",
    "                     for v in d['FORM'].values()])),\n",
    "            len(lenvalid)))\n",
    "\n",
    "print(\n",
    "'''Test:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbtest[min(dbtest)]) + 1,\n",
    "            sum([len(d['ARG']) for p, d in dbtest.items()]), \n",
    "            len(set([v\n",
    "                     for p, d in dbtest.items()\n",
    "                     for v in d['FORM'].values()])),\n",
    "            len(lentest)))\n",
    "\n",
    "print(\n",
    "'''Overall:\n",
    "  \\tattributes:{:}\\trecords:{:07d}\\tvocab:{:07d}\\tpropositions:{:}'''\n",
    "    .format(len(dbtrain[1]) + 1, \n",
    "            sum([len(d['ARG']) for p, d in dbtrain.items()] +\n",
    "                [len(d['ARG']) for p, d in dbvalid.items()] +\n",
    "                [len(d['ARG']) for p, d in dbtest.items()]), \n",
    "            len(set([v                    \n",
    "                     for p, d in dbtrain.items()\n",
    "                     for v in d['FORM'].values()] +\n",
    "                    [v                    \n",
    "                     for p, d in dbvalid.items()\n",
    "                     for v in d['FORM'].values()] +\n",
    "                    [v\n",
    "                     for p, d in dbtest.items()  \n",
    "                     for v in d['FORM'].values()])),\n",
    "             len(lentrain) + len(lenvalid) + len(lentest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Numpfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbtrain = numpfy_propositions_dict(dbtrain, lentrain)\n",
    "dbvalid = numpfy_propositions_dict(dbvalid, lenvalid)\n",
    "dbtest = numpfy_propositions_dict(dbtest, lentest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(db1, propid):\n",
    "    '''Generate inputs\n",
    "    '''\n",
    "    propdb = db1[propid] # nested dict of columns and idx value\n",
    "    try:\n",
    "        if 'CHUNK_SPACE' not in propdb:\n",
    "            proplen = len(propdb['ID'])\n",
    "            propdb['CHUNK_SPACE'] = generate_chunk_space(proplen)\n",
    "\n",
    "        # Replaces word with tokens\n",
    "        word    = propdb['FORM']\n",
    "        ctx_p_left  = propdb['FORM_CTX_P-1']\n",
    "        ctx_p0  = propdb['FORM_CTX_P+0']\n",
    "        ctx_p_right  = propdb['FORM_CTX_P+1']\n",
    "\n",
    "        marker  = propdb['MARKER']\n",
    "        pos     = propdb['GPOS']\n",
    "        chunk_type  = propdb['T']\n",
    "        chunk_start, chunk_finish = propdb['CHUNK_SPACE']\n",
    "    except KeyError:\n",
    "        import code; code.interact(local=dict(globals(), **locals()))\n",
    "\n",
    "    return word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish\n",
    "            \n",
    "def generate_chunk_space(n):\n",
    "    '''Generates all possible spaces for chunks\n",
    "    '''\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    for i in range(n):\n",
    "        for j in range(i,n,1):\n",
    "            start_list.append(i)\n",
    "            end_list.append(j+1)\n",
    "    shape_ = (len(start_list), 1)\n",
    "    start_ = np.array(start_list).reshape(shape_)\n",
    "    finish_ = np.array(end_list).reshape(shape_)\n",
    "    return start_, finish_\n",
    "            \n",
    "\n",
    "def get_outputs(db1, propid, n_targets):\n",
    "    ''' Generate outputs\n",
    "    '''\n",
    "    return db1[propid]['IOB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795 ns ± 5.46 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "propid = 1120\n",
    "word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish = get_inputs(dbtrain, propid)\n",
    "targets = get_outputs(dbtrain, propid, n_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('B-A0', 0), ('B-A1', 1), ('B-A2', 2), ('B-A3', 3), ('B-A4', 4), ('B-AM-ADV', 5), ('B-AM-CAU', 6), ('B-AM-DIR', 7), ('B-AM-DIS', 8), ('B-AM-EXT', 9), ('B-AM-LOC', 10), ('B-AM-MNR', 11), ('B-AM-NEG', 12), ('B-AM-PNC', 13), ('B-AM-PRD', 14), ('B-AM-REC', 15), ('B-AM-TMP', 16), ('B-V', 17), ('I-A0', 18), ('I-A1', 19), ('I-A2', 20), ('I-A3', 21), ('I-A4', 22), ('I-AM-ADV', 23), ('I-AM-CAU', 24), ('I-AM-DIR', 25), ('I-AM-DIS', 26), ('I-AM-EXT', 27), ('I-AM-LOC', 28), ('I-AM-MNR', 29), ('I-AM-NEG', 30), ('I-AM-PNC', 31), ('I-AM-PRD', 32), ('I-AM-REC', 33), ('I-AM-TMP', 34), ('I-V', 35), ('O', 36)])\n",
      "[[36]\n",
      " [36]\n",
      " [36]\n",
      " [36]\n",
      " [36]\n",
      " [36]\n",
      " [36]\n",
      " [17]\n",
      " [10]\n",
      " [28]\n",
      " [28]\n",
      " [28]\n",
      " [28]\n",
      " [28]\n",
      " [36]\n",
      " [36]\n",
      " [36]\n",
      " [36]]\n"
     ]
    }
   ],
   "source": [
    "propid = 1120\n",
    "labels = get_outputs(dbtrain, propid, n_targets)\n",
    "print(lex2idx['IOB'])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>P</th>\n",
       "      <th>FORM</th>\n",
       "      <th>GPOS</th>\n",
       "      <th>MARKER</th>\n",
       "      <th>ARG</th>\n",
       "      <th>T</th>\n",
       "      <th>IOB</th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>CHUNK_START</th>\n",
       "      <th>CHUNK_FINISH</th>\n",
       "      <th>CHUNK_LEN</th>\n",
       "      <th>CHUNK_CANDIDATE_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDEX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27125</th>\n",
       "      <td>1</td>\n",
       "      <td>1120</td>\n",
       "      <td>Devo</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27126</th>\n",
       "      <td>2</td>\n",
       "      <td>1120</td>\n",
       "      <td>fingir</td>\n",
       "      <td>V-INF</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27127</th>\n",
       "      <td>3</td>\n",
       "      <td>1120</td>\n",
       "      <td>que</td>\n",
       "      <td>CONJ-S</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27128</th>\n",
       "      <td>4</td>\n",
       "      <td>1120</td>\n",
       "      <td>enxergo</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27129</th>\n",
       "      <td>5</td>\n",
       "      <td>1120</td>\n",
       "      <td>a</td>\n",
       "      <td>ART</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27130</th>\n",
       "      <td>6</td>\n",
       "      <td>1120</td>\n",
       "      <td>figura</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27131</th>\n",
       "      <td>7</td>\n",
       "      <td>1120</td>\n",
       "      <td>ou</td>\n",
       "      <td>CONJ-C</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>5999</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27132</th>\n",
       "      <td>8</td>\n",
       "      <td>1120</td>\n",
       "      <td>sento</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>1</td>\n",
       "      <td>(V*)</td>\n",
       "      <td>V</td>\n",
       "      <td>B-V</td>\n",
       "      <td>6000</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27133</th>\n",
       "      <td>9</td>\n",
       "      <td>1120</td>\n",
       "      <td>a</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1</td>\n",
       "      <td>(AM-LOC*</td>\n",
       "      <td>AM-LOC</td>\n",
       "      <td>B-AM-LOC</td>\n",
       "      <td>6001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27134</th>\n",
       "      <td>10</td>\n",
       "      <td>1120</td>\n",
       "      <td>as</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>AM-LOC</td>\n",
       "      <td>I-AM-LOC</td>\n",
       "      <td>6001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27135</th>\n",
       "      <td>11</td>\n",
       "      <td>1120</td>\n",
       "      <td>margens</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>AM-LOC</td>\n",
       "      <td>I-AM-LOC</td>\n",
       "      <td>6001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27136</th>\n",
       "      <td>12</td>\n",
       "      <td>1120</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>AM-LOC</td>\n",
       "      <td>I-AM-LOC</td>\n",
       "      <td>6001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27137</th>\n",
       "      <td>13</td>\n",
       "      <td>1120</td>\n",
       "      <td>o</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>AM-LOC</td>\n",
       "      <td>I-AM-LOC</td>\n",
       "      <td>6001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27138</th>\n",
       "      <td>14</td>\n",
       "      <td>1120</td>\n",
       "      <td>Rio_Piedra</td>\n",
       "      <td>PROP</td>\n",
       "      <td>1</td>\n",
       "      <td>*)</td>\n",
       "      <td>AM-LOC</td>\n",
       "      <td>I-AM-LOC</td>\n",
       "      <td>6001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27139</th>\n",
       "      <td>15</td>\n",
       "      <td>1120</td>\n",
       "      <td>e</td>\n",
       "      <td>CONJ-C</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>6002</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27140</th>\n",
       "      <td>16</td>\n",
       "      <td>1120</td>\n",
       "      <td>choro</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>6002</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27141</th>\n",
       "      <td>17</td>\n",
       "      <td>1120</td>\n",
       "      <td>?</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>6002</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27142</th>\n",
       "      <td>18</td>\n",
       "      <td>1120</td>\n",
       "      <td>»</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>O</td>\n",
       "      <td>6002</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID     P        FORM    GPOS  MARKER       ARG       T       IOB  \\\n",
       "INDEX                                                                     \n",
       "27125   1  1120        Devo   V-FIN       0         *       *         O   \n",
       "27126   2  1120      fingir   V-INF       0         *       *         O   \n",
       "27127   3  1120         que  CONJ-S       0         *       *         O   \n",
       "27128   4  1120     enxergo   V-FIN       0         *       *         O   \n",
       "27129   5  1120           a     ART       0         *       *         O   \n",
       "27130   6  1120      figura       N       0         *       *         O   \n",
       "27131   7  1120          ou  CONJ-C       0         *       *         O   \n",
       "27132   8  1120       sento   V-FIN       1      (V*)       V       B-V   \n",
       "27133   9  1120           a     PRP       1  (AM-LOC*  AM-LOC  B-AM-LOC   \n",
       "27134  10  1120          as     ART       1         *  AM-LOC  I-AM-LOC   \n",
       "27135  11  1120     margens       N       1         *  AM-LOC  I-AM-LOC   \n",
       "27136  12  1120          de     PRP       1         *  AM-LOC  I-AM-LOC   \n",
       "27137  13  1120           o     ART       1         *  AM-LOC  I-AM-LOC   \n",
       "27138  14  1120  Rio_Piedra    PROP       1        *)  AM-LOC  I-AM-LOC   \n",
       "27139  15  1120           e  CONJ-C       1         *       *         O   \n",
       "27140  16  1120       choro   V-FIN       1         *       *         O   \n",
       "27141  17  1120           ?      PU       1         *       *         O   \n",
       "27142  18  1120           »      PU       1         *       *         O   \n",
       "\n",
       "       CHUNK_ID  CHUNK_START  CHUNK_FINISH  CHUNK_LEN  CHUNK_CANDIDATE_ID  \n",
       "INDEX                                                                      \n",
       "27125      5999            0             7          7                   6  \n",
       "27126      5999            0             7          7                   6  \n",
       "27127      5999            0             7          7                   6  \n",
       "27128      5999            0             7          7                   6  \n",
       "27129      5999            0             7          7                   6  \n",
       "27130      5999            0             7          7                   6  \n",
       "27131      5999            0             7          7                   6  \n",
       "27132      6000            7             8          1                 105  \n",
       "27133      6001            8            14          6                 121  \n",
       "27134      6001            8            14          6                 121  \n",
       "27135      6001            8            14          6                 121  \n",
       "27136      6001            8            14          6                 121  \n",
       "27137      6001            8            14          6                 121  \n",
       "27138      6001            8            14          6                 121  \n",
       "27139      6002           14            18          4                 164  \n",
       "27140      6002           14            18          4                 164  \n",
       "27141      6002           14            18          4                 164  \n",
       "27142      6002           14            18          4                 164  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfgs[dfgs['P'] == 1120]\n",
    "df[DISPLAY_COLUMNS].head(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Viterbi Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_path(t_out, t_edge_scores, n_tags):\n",
    "    \n",
    "    def step(prev, et):\n",
    "        # last computed scores and last computed transitions\n",
    "        prev_scores, prev_selections = prev\n",
    "        \n",
    "        current_scores = tf.transpose(prev_scores + et)\n",
    "\n",
    "        best_scores = tf.reduce_max(current_scores, axis=0)\n",
    "        best_options = tf.argmax(current_scores, axis=0)\n",
    "\n",
    "        return best_scores, best_options\n",
    "    \n",
    "    score_matrix, selection_matrix = tf.scan(\n",
    "        fn=step,\n",
    "        elems=t_edge_scores,\n",
    "        initializer=(tf.zeros(n_tags), tf.to_int64(tf.zeros(n_tags))),\n",
    "    )\n",
    "    \n",
    "    return score_matrix, selection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_path(selection_matrix, last_tag):\n",
    "    \n",
    "    def step(prev, t):\n",
    "        selection_matrix, prev_best = prev\n",
    "\n",
    "        current = selection_matrix[t][prev_best]\n",
    "\n",
    "        return selection_matrix, current\n",
    "\n",
    "    m = tf.shape(selection_matrix)[0]\n",
    "    _, rev_path = tf.scan(\n",
    "        fn = step,\n",
    "        elems=m-1-tf.range(m),\n",
    "        initializer=(selection_matrix, last_tag)\n",
    "\n",
    "    )\n",
    "\n",
    "    best_path = tf.concat((tf.reverse(rev_path,axis=[0]),[last_tag]),axis=0)\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_score(t_edge_scores, path, n):\n",
    "    \n",
    "    def step(prev, t):\n",
    "        edge_scores, path, prev_score = prev\n",
    "        \n",
    "        transition_score = edge_scores[t]\n",
    "        \n",
    "        p_t = path[t]\n",
    "        p_tp1 = path[t+1]\n",
    "\n",
    "        current_score = transition_score[p_tp1,p_t] + prev_score\n",
    "\n",
    "        return edge_scores, path, current_score\n",
    "\n",
    "    _, _, path_score = tf.scan(\n",
    "    fn = step,\n",
    "    elems=tf.range(n-1),\n",
    "    initializer=(t_edge_scores, path, tf.zeros(1))\n",
    "    )\n",
    "    return path_score[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'hidden_features':200,\n",
    "    'state_size':200,\n",
    "    'learning_rate':0.001,\n",
    "    'spn_layer':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(hparams):\n",
    "\n",
    "    # Determine parameters for the computation graph\n",
    "#     vocab_size = len(set(lex2tok.values()))\n",
    "#     capt_size = model_meta['capt_size']\n",
    "    pos_size = len(lex2idx['GPOS'])\n",
    "#     char_size = model_meta['char_size']\n",
    "    ck_size = len(lex2idx['IOB'])\n",
    "    embed_size = embeddings.shape[1]\n",
    "#     capt_embed_size = model_meta['capt_embed_size']\n",
    "#     char_embed_size = model_meta['char_embed_size']\n",
    "#     char_hidden = model_meta['char_hidden_features']\n",
    "    pos_embed_size = len(idx2lex['GPOS'])\n",
    "\n",
    "    hidden_features = hparams['hidden_features']\n",
    "    state_size = hparams['state_size']\n",
    "    lr = hparams['learning_rate']\n",
    "    spn_layer = hparams['spn_layer']\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "# word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, \n",
    "\n",
    "    t_x_words = tf.placeholder(tf.int32,(None,1))   # ids form\n",
    "    t_x_ctx_p_left = tf.placeholder(tf.int32,(None,1))\n",
    "    t_x_ctx_p0 = tf.placeholder(tf.int32,(None,1)) \n",
    "    t_x_ctx_p_right = tf.placeholder(tf.int32,(None,1))\n",
    "    \n",
    "    t_x_pos = tf.placeholder(tf.int32,(None,1))     # ids POS\n",
    "    t_x_marker = tf.placeholder(tf.int32, (None,1))\n",
    "    \n",
    "    t_y_ck = tf.placeholder(tf.int32, shape=(None,)) # ids IOB-SRL\n",
    "    \n",
    "    \n",
    "    t_inputs = [t_x_words, t_x_ctx_p_left, t_x_ctx_p0, t_x_ctx_p_right, t_x_marker, t_x_pos]\n",
    "    t_targets = [t_y_ck]\n",
    "\n",
    "    with tf.variable_scope('Feature_Vars'):\n",
    "        t_W_embed = tf.Variable(embeddings.astype(np.float32))\n",
    "#         t_W_char = tf.Variable(np.random.normal(0,0.1,(char_size+1, char_embed_size)).astype(np.float32))\n",
    "\n",
    "        t_gamma = tf.Variable(np.random.normal(0,1.0, 1).astype(np.float32))\n",
    "        t_beta = tf.Variable(np.random.normal(0,1.0, 1).astype(np.float32))\n",
    "\n",
    "        dim = state_size\n",
    "        if dim is None:\n",
    "            dim = 2*hidden_features\n",
    "        if spn_layer:\n",
    "            t_W_tran = tf.Variable(np.random.normal(0,1.0/np.sqrt(3*dim),(3*dim, ck_size*ck_size)).astype(np.float32))\n",
    "        else:\n",
    "            t_W_ck = tf.Variable(np.random.normal(0,0.1,(dim, ck_size)).astype(np.float32))\n",
    "        \n",
    "\n",
    "\n",
    "    t_words = tf.gather_nd(t_W_embed, t_x_words)\n",
    "    t_ctx_p_left = tf.gather_nd(t_W_embed, t_x_ctx_p_left)\n",
    "    t_ctx_p0 = tf.gather_nd(t_W_embed, t_x_ctx_p0)\n",
    "    t_ctx_p_right = tf.gather_nd(t_W_embed, t_x_ctx_p_right)\n",
    "                                     \n",
    "    t_x_pos_flat = tf.squeeze(t_x_pos, axis=1)\n",
    "\n",
    "    t_pos = tf.to_float(tf.one_hot(indices= t_x_pos_flat, depth=pos_size, on_value=1,off_value=0))\n",
    "    t_marker = tf.cast(t_x_marker, tf.float32)\n",
    "    \n",
    "    t_word_feats = tf.concat((t_words, t_ctx_p_left, t_ctx_p0, t_ctx_p_right, t_marker, t_pos), axis=1)\n",
    "    print(t_word_feats.get_shape())\n",
    "\n",
    "    t_sq_len = tf.shape(t_x_words)[0]\n",
    "    print(t_sq_len)\n",
    "\n",
    "    n_features = embed_size\n",
    "\n",
    "    t_words_shp = tf.reshape(\n",
    "        t_word_feats, (1,t_sq_len, pos_size + embed_size * 4 + 1)\n",
    "    )\n",
    "\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=hidden_features, state_is_tuple=True)\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=hidden_features, state_is_tuple=True)\n",
    "\n",
    "    with tf.variable_scope(\"Bilstm\"):\n",
    "        t_h1, t_last_states =tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            dtype=tf.float32,\n",
    "            inputs=t_words_shp)\n",
    "\n",
    "        t_hidden = tf.concat((t_h1[0][0],t_h1[1][0]),axis=1)\n",
    "\n",
    "    if state_size is not None:\n",
    "        t_lstmcell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "        t_hidden_shp = tf.reshape(t_hidden, (1,t_sq_len, 2*hidden_features))\n",
    "\n",
    "        with tf.variable_scope('LSTM_last'):\n",
    "            t_h2, t_last_states2 =tf.nn.dynamic_rnn(\n",
    "                cell=t_lstmcell,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=[t_sq_len],\n",
    "                inputs=t_hidden_shp)\n",
    "\n",
    "        t_out = t_h2[0]\n",
    "    else:\n",
    "        t_out = t_hidden\n",
    "        \n",
    "    n = tf.shape(t_out)[0]\n",
    "    \n",
    "    t_outputs = []\n",
    "    if spn_layer:\n",
    "        t_edges = tf.concat((t_out[1:],t_out[:-1],t_out[1:]*t_out[:-1]),axis=1)\n",
    "        t_edge_scores = tf.matmul(t_edges, t_W_tran)\n",
    "\n",
    "        t_edge_scores = tf.reshape(t_edge_scores, ((n-1)*ck_size*ck_size,))\n",
    "\n",
    "        # Batch Normalization\n",
    "        t_es_mean = tf.reduce_mean(t_edge_scores)\n",
    "        t_es_m2 = tf.reduce_mean(t_edge_scores**2)\n",
    "\n",
    "        t_es_var = t_es_m2 - t_es_mean**2\n",
    "        t_es_std = tf.sqrt(t_es_var + 1e-8)\n",
    "\n",
    "        t_es_norm = (t_edge_scores - t_es_mean)/t_es_std\n",
    "\n",
    "        t_es_renorm = t_gamma * t_es_norm + t_beta\n",
    "\n",
    "        t_edge_scores = tf.reshape(t_es_renorm, (n-1,ck_size,ck_size))\n",
    "\n",
    "        t_score_matrix, t_selection_matrix = longest_path(t_out, t_edge_scores,ck_size)\n",
    "\n",
    "        t_best_score = tf.reduce_max(t_score_matrix[-1])\n",
    "        t_last_tag = tf.argmax(t_score_matrix[-1])\n",
    "\n",
    "        t_best_path = retrieve_path(t_selection_matrix, t_last_tag)\n",
    "\n",
    "        t_correct_score = path_score(t_edge_scores, t_y_ck, n)\n",
    "\n",
    "        t_cost = -t_correct_score\n",
    "\n",
    "        # # gradiente descendente no custo do perceptron estruturado\n",
    "        t_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        t_train = t_optimizer.minimize(t_cost)\n",
    "\n",
    "        t_outputs.extend([t_score_matrix, t_best_path])\n",
    "        \n",
    "        # word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, _, _, _ = get_inputs(dbtrain, sample)\n",
    "        def t_pred(sess, x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos):\n",
    "            result = sess.run(t_best_path, feed_dict={\n",
    "                t_inputs[0]:x_word,\n",
    "                t_inputs[1]:x_ctx_p_left,\n",
    "                t_inputs[2]:x_ctx_p0,\n",
    "                t_inputs[3]:x_ctx_p_right,                \n",
    "                t_inputs[4]:x_marker,\n",
    "                t_inputs[5]:x_pos\n",
    "            })\n",
    "            return result\n",
    "\n",
    "        # word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, _, _, _ = get_inputs(dbtrain, sample)\n",
    "        def my_t_train(sess, word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, y_ck):\n",
    "            _, result = sess.run([t_train, t_cost], feed_dict={\n",
    "                t_inputs[0]:x_word,\n",
    "                t_inputs[1]:x_ctx_p_left,\n",
    "                t_inputs[2]:x_ctx_p0,\n",
    "                t_inputs[3]:x_ctx_p_right,                \n",
    "                t_inputs[4]:x_marker,\n",
    "                t_inputs[5]:x_pos,\n",
    "                t_targets[0]:y_ck\n",
    "            })\n",
    "            return result\n",
    "    else: #CRF\n",
    "        t_ck_score = tf.matmul(t_out,t_W_ck)\n",
    "\n",
    "        t_ck_score_ext = tf.expand_dims(t_ck_score, 0)\n",
    "        t_y_ck_ext = tf.expand_dims(t_y_ck, 0)\n",
    "\n",
    "        t_sequence_lengths = tf.shape(t_x_words)[0]\n",
    "        t_sequence_lengths = tf.expand_dims(t_sequence_lengths,0)\n",
    "\n",
    "        t_log_likelihood, t_transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            t_ck_score_ext, \n",
    "            t_y_ck_ext, \n",
    "            t_sequence_lengths)\n",
    "        \n",
    "        t_outputs.extend([t_ck_score, t_transition_params])\n",
    "    \n",
    "        t_cost = -t_log_likelihood\n",
    "\n",
    "        # # gradiente descendente no custo do perceptron estruturado\n",
    "        t_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(0.003)\n",
    "        t_train = t_optimizer.minimize(t_cost)\n",
    "\n",
    "#         def t_pred(sess, inputs, x_words, x_capt, x_pos, x_char_matrix, x_char_lens):\n",
    "        def t_pred(sess, x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos):\n",
    "            score, tparams = sess.run([t_ck_score,t_transition_params], feed_dict={\n",
    "                t_inputs[0]:x_word,\n",
    "                t_inputs[1]:x_ctx_p_left,\n",
    "                t_inputs[2]:x_ctx_p0,\n",
    "                t_inputs[3]:x_ctx_p_right,                \n",
    "                t_inputs[4]:x_marker,\n",
    "                t_inputs[5]:x_pos\n",
    "            })\n",
    "\n",
    "            return tf.contrib.crf.viterbi_decode(score=score,transition_params=tparams)[0][:-1]\n",
    "        \n",
    "        # word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, _, _, _ = get_inputs(dbtrain, sample)\n",
    "        def my_t_train(sess, word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, y_ck):\n",
    "            _, result = sess.run([t_train, t_cost], feed_dict={\n",
    "                t_inputs[0]:x_word,\n",
    "                t_inputs[1]:x_ctx_p_left,\n",
    "                t_inputs[2]:x_ctx_p0,\n",
    "                t_inputs[3]:x_ctx_p_right,                \n",
    "                t_inputs[4]:x_marker,\n",
    "                t_inputs[5]:x_pos,\n",
    "                t_targets[0]:y_ck\n",
    "            })\n",
    "            return result\n",
    "    \n",
    "    return t_inputs, t_targets, t_outputs, my_t_train, t_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 227)\n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t_inputs, t_targets, t_outputs, t_train, t_pred = make_graph(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6\n",
      " 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6\n",
      " 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6 34  6\n",
      " 34]\n",
      "(73, 1)\n",
      "['I-AM-PNC', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP', 'B-AM-CAU', 'I-AM-TMP']\n"
     ]
    }
   ],
   "source": [
    "sample = 146\n",
    "# word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish\n",
    "x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, _, _, _ = get_inputs(dbtrain, sample)\n",
    "\n",
    "y_ck = get_outputs(dbtrain, sample, n_targets)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    o_ck = t_pred(sess, x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos)\n",
    "print(o_ck)\n",
    "print(y_ck.shape)\n",
    "print([idx2lex['IOB'][ck] for ck in list(o_ck)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Overfit one proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.620975]\n",
      "[-1972.0256]\n",
      "[-2385.631]\n"
     ]
    }
   ],
   "source": [
    "sample =146\n",
    "# word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish\n",
    "# x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, _, _, _ = get_inputs(dbtrain, sample)\n",
    "x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, _, _, _ = get_inputs(dbtrain, sample)\n",
    "y_ck = get_outputs(dbtrain, sample, n_targets)\n",
    "y_ck = np.squeeze(y_ck, axis=1)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(200):\n",
    "    L = t_train(sess, x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, y_ck)\n",
    "    if i % 100 == 0:\n",
    "        print(L)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Evaluate one proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_conll(sess, prop_dict, propid, idx2lex):\n",
    "    gold_list = []\n",
    "    eval_list = []\n",
    "        \n",
    "    x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, _, _, _ = get_inputs(prop_dict, propid)\n",
    "    targets = get_outputs(prop_dict, propid, n_targets)\n",
    "    predictions = t_pred(sess, x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos)\n",
    "\n",
    "\n",
    "\n",
    "    n_words = len(x_word)\n",
    "    default_ck_list_ = [(i,i + 1, '*') for i in range(n_words)]\n",
    "    \n",
    "    pred_array = prop_dict[propid]['PRED']\n",
    "    pred_array = pred_array.flatten()\n",
    "    \n",
    "    arg_array = prop_dict[propid]['ARG']\n",
    "    arg_array = arg_array.flatten()\n",
    "    \n",
    "    pred_list = [idx2lex['PRED'][i] for i in pred_array.tolist()]\n",
    "    gold_list_ = [idx2lex['ARG'][i] for i in arg_array.tolist()]\n",
    "     \n",
    "    gold_list += list(zip(pred_list, gold_list_))\n",
    "    prop_list =  [propid for _ in range(len(gold_list))]\n",
    "    eval_list = [idx2lex['IOB'][i] for i in predictions]\n",
    "    \n",
    "    #FIXME: removed #end of sentence token\n",
    "    # now predictions are shorter then gold\n",
    "    if len(eval_list) < len(prop_list):\n",
    "        eval_list.append('O')\n",
    "\n",
    "    eval_list = propbankbr_iob2arg(prop_list, eval_list)\n",
    "    \n",
    "#     ck_list_ = []     \n",
    "#     for triple_ in sorted(chunk_ext, key= lambda x: x[0]):\n",
    "#         lb, ub, ckid = triple_\n",
    "#         # filters default value\n",
    "#         default_ck_list_ = [\n",
    "#             dck_\n",
    "#             for dck_ in default_ck_list_ if dck_[0] < lb or dck_[1] > ub\n",
    "#         ]\n",
    "#         ck_list_.append((lb, ub, idx2lex['T'][ckid]))        \n",
    "\n",
    "#     ck_list_ = default_ck_list_ + ck_list_ \n",
    "\n",
    "#     arg_list_ = []\n",
    "#     for triple_ in sorted(ck_list_, key= lambda x: x[0]):\n",
    "#         lb, ub, cktype = triple_\n",
    "#         flat_list_ = [ cktype if i == lb else '*' for i in range(lb, ub) ]\n",
    "            \n",
    "#         if cktype != '*':\n",
    "#             flat_list_[0] = '({:}*'.format(flat_list_[0])\n",
    "#             flat_list_[-1] = '{:})'.format(flat_list_[-1])\n",
    "#         arg_list_ += flat_list_\n",
    "        \n",
    "    eval_list = list(zip(pred_list, eval_list))\n",
    "#     eval_list.append(None)\n",
    "#     gold_list.append(None)\n",
    "#     Change to use CoNLL2004\n",
    "    gold_list = propbankbr_arg2se(gold_list)\n",
    "    eval_list = propbankbr_arg2se(eval_list)\n",
    "\n",
    "    return gold_list, eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold_list, eval_list, ds_type='train',verbose=True):\n",
    "    gold_path = '{}_gold.props'.format(ds_type)    \n",
    "    eval_path = '{}_eval.props'.format(ds_type)\n",
    "\n",
    "    with open(gold_path, mode='w') as f:        \n",
    "        for tuple_ in gold_list:\n",
    "            if tuple_ is None:\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write('{:}\\t{:}\\n'.format(*tuple_))\n",
    "\n",
    "    with open(eval_path, mode='w') as f:        \n",
    "        for tuple_ in eval_list:\n",
    "            if tuple_ is None:\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write('{:}\\t{:}\\n'.format(*tuple_))\n",
    "\n",
    "    pipe = Popen(['perl',PEARL_SRLEVAL_PATH, gold_path, eval_path], stdout=PIPE, stderr=PIPE)\n",
    "\n",
    "    txt, err = pipe.communicate()\n",
    "    txt = txt.decode('UTF-8')\n",
    "    err = err.decode('UTF-8')\n",
    "    \n",
    "    print(err)\n",
    "    if verbose:\n",
    "        print(txt)\n",
    "        with open('{}.conll'.format(ds_type), mode='w') as f:\n",
    "            f.write(txt)\n",
    "\n",
    "    # overall is a summary from the list\n",
    "    # is the seventh line\n",
    "    lines_list = txt.split('\\n')        \n",
    "    \n",
    "    # get the numbers from the row \n",
    "    overall_list = re.findall(r'[-+]?[0-9]*\\.?[0-9]+.', lines_list[6])\n",
    "    f1 = float(overall_list[-1])\n",
    "\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(sess, prop_dict, idx2lex, ds_type='train'):\n",
    "    gold_list = []\n",
    "    eval_list = []\n",
    "    first = True\n",
    "    for pid in prop_dict:        \n",
    "        g_list, e_list = tag_to_conll(sess, prop_dict, pid, idx2lex)\n",
    "\n",
    "        if not first:\n",
    "            gold_list.append(None)\n",
    "            eval_list.append(None)\n",
    "        else:\n",
    "            first = False\n",
    "\n",
    "        gold_list += g_list\n",
    "        eval_list += e_list\n",
    "\n",
    "    return evaluate(gold_list, eval_list, ds_type=ds_type, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Sentences    :           1\n",
      "Number of Propositions :           1\n",
      "Percentage of perfect props : 100.00\n",
      "\n",
      "              corr.  excess  missed    prec.    rec.      F1\n",
      "------------------------------------------------------------\n",
      "   Overall        4       0       0   100.00  100.00  100.00\n",
      "----------\n",
      "        A1        2       0       0   100.00  100.00  100.00\n",
      "    AM-ADV        1       0       0   100.00  100.00  100.00\n",
      "    AM-NEG        1       0       0   100.00  100.00  100.00\n",
      "------------------------------------------------------------\n",
      "         V        2       0       0   100.00  100.00  100.00\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_tags, eval_tags = tag_to_conll(sess, dbtrain, sample, idx2lex)\n",
    "f1 = evaluate(gold_tags, eval_tags, 'prop_{:}'.format(sample), verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Sentences    :        5295\n",
      "Number of Propositions :        5295\n",
      "Percentage of perfect props :   0.21\n",
      "\n",
      "              corr.  excess  missed    prec.    rec.      F1\n",
      "------------------------------------------------------------\n",
      "   Overall      129   10616   12380     1.20    1.03    1.11\n",
      "----------\n",
      "        A0        0       0    2816     0.00    0.00    0.00\n",
      "        A1       85    5706    4705     1.47    1.77    1.61\n",
      "        A2        0       0    1027     0.00    0.00    0.00\n",
      "        A3        0       0     106     0.00    0.00    0.00\n",
      "        A4        0       0      66     0.00    0.00    0.00\n",
      "    AM-ADV       17    1876     321     0.90    5.03    1.52\n",
      "    AM-CAU        0       0     149     0.00    0.00    0.00\n",
      "    AM-DIR        0       0      12     0.00    0.00    0.00\n",
      "    AM-DIS        0       0     271     0.00    0.00    0.00\n",
      "    AM-EXT        0       0      78     0.00    0.00    0.00\n",
      "    AM-LOC        0       0     721     0.00    0.00    0.00\n",
      "    AM-MNR        0       0     377     0.00    0.00    0.00\n",
      "    AM-NEG       27    3034     275     0.88    8.94    1.61\n",
      "    AM-PNC        0       0     159     0.00    0.00    0.00\n",
      "    AM-PRD        0       0     179     0.00    0.00    0.00\n",
      "    AM-REC        0       0      60     0.00    0.00    0.00\n",
      "    AM-TMP        0       0    1058     0.00    0.00    0.00\n",
      "------------------------------------------------------------\n",
      "         V      512    4840    4975     9.57    9.33    9.45\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_train = evaluate_dataset(sess, dbtrain, idx2lex, ds_type='train')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino Parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_meta = {\n",
    "#     'vocab_size':len(id_to_word),\n",
    "#     'capt_size':len(id_to_capt),\n",
    "#     'pos_size':len(id_to_pos),\n",
    "#     'char_size':len(id_to_char),\n",
    "#     'ck_size':len(id_to_ck),\n",
    "#     'embed_size':embedding_matrix.shape[1],\n",
    "#     'capt_embed_size':10,\n",
    "#     'char_hidden_features':30,\n",
    "#     'char_embed_size':30,\n",
    "#     'pos_embed_size':10,\n",
    "#     'hidden_features':200,\n",
    "#     'state_size':200,\n",
    "#     'learning_rate':0.001,\n",
    "#     'spn_layer':True\n",
    "# }\n",
    "\n",
    "# t_inputs, t_targets, t_outputs, t_train, t_pred = make_model(model_meta, embedding_matrix)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# n_epochs = 3\n",
    "\n",
    "# indices = np.arange(len(train_sentences)//100)    \n",
    "# n = len(indices)\n",
    "\n",
    "# best_path = None\n",
    "# best_f1 = 0\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     j = 0\n",
    "#     np.random.shuffle(indices)\n",
    "#     for sid in indices:\n",
    "#         j += 1\n",
    "#         x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(train_sentences[sid])\n",
    "#         y_ck = get_output(train_sentences[sid])\n",
    "#         L = t_train(sess,t_inputs,t_targets,x_words,x_capt,x_pos,x_char_matrix,x_lens,y_ck)\n",
    "#         if j % (n//10) == 0:\n",
    "#             print('{:.2f} %'.format(100*j/n))\n",
    "        \n",
    "#     print(\"Epoca \", (epoch+1))\n",
    "#     dev_f1 = evaluate_conll(sess, df_ck, dev_sentences[:len(indices)],t_inputs,t_pred,verbose=False)\n",
    "#     print(\"dev f1: \", dev_f1)\n",
    "#     train_f1 = evaluate_conll(sess, df_ck, train_sentences[:len(indices)],t_inputs,t_pred,verbose=False)\n",
    "#     print(\"train f1: \", train_f1)\n",
    "    \n",
    "#     if dev_f1 > best_f1:\n",
    "#         best_f1 = dev_f1\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "train_sentences = range(*get_db_bounds('train'))\n",
    "dev_sentences = range(*get_db_bounds('valid'))\n",
    "test_sentences = range(*get_db_bounds('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   3\n",
      "Training Model {'hidden_features': 50, 'state_size': 50, 'learning_rate': 0.0005, 'spn_layer': True}\n",
      "(?, 227)\n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "lr_search = [0.0005]\n",
    "hidden_features_search = [50,150,250]\n",
    "\n",
    "for j in range(len(hidden_features_search)):\n",
    "    for i in range(len(lr_search)):\n",
    "        for s in range(2):\n",
    "            lr = lr_search[i]\n",
    "            hf = hidden_features_search[j]\n",
    "\n",
    "            print(j, ' ', len(hidden_features_search))\n",
    "#             model_meta = {\n",
    "#                 'vocab_size':len(id_to_word),\n",
    "#                 'capt_size':len(id_to_capt),\n",
    "#                 'pos_size':len(id_to_pos),\n",
    "#                 'char_size':len(id_to_char),\n",
    "#                 'ck_size':len(id_to_ck),\n",
    "#                 'embed_size':embedding_matrix.shape[1],\n",
    "#                 'capt_embed_size':10,\n",
    "#                 'char_embed_size':30,\n",
    "#                 'pos_embed_size':10,\n",
    "#                 'char_hidden_features':30,\n",
    "#                 'hidden_features':hf,\n",
    "#                 'state_size':hf,\n",
    "#                 'learning_rate':lr,\n",
    "#                 'spn_layer':s==0\n",
    "#             }\n",
    "            model_meta = {\n",
    "                'hidden_features':hf,\n",
    "                'state_size':hf,\n",
    "                'learning_rate':lr,\n",
    "                'spn_layer':s==0\n",
    "            }\n",
    "            print('Training Model', model_meta)\n",
    "            \n",
    "            last_layer = 'crf'\n",
    "            if s == 0:\n",
    "                last_layer = 'spn'\n",
    "\n",
    "            n_epochs = 50\n",
    "            model_name = 'bilstm_viterbi_h' + str(hf) + '_lr_' + str(lr) + '_' + last_layer\n",
    "            model = make_graph(model_meta)\n",
    "\n",
    "\n",
    "            t_inputs, t_targets, t_outputs, t_train, t_pred = model\n",
    "            \n",
    "            #NEW SESSION PER MODEL\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            sess = tf.Session(config=config)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            indices = np.arange(1, len(train_sentences))\n",
    "            n = len(indices)\n",
    "\n",
    "            best_path = None\n",
    "            best_f1 = 0\n",
    "\n",
    "            save_dir = '../outputs/1.0/notebooks/spn/models/'\n",
    "            exp_desc_dir = '{}results/'.format(save_dir)\n",
    "            saver = tf.train.Saver(max_to_keep=n_epochs*20)\n",
    "\n",
    "            dev_f1_list = []\n",
    "            train_f1_list = []\n",
    "            consecutive_bad_dev = 0\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                start = time.time()\n",
    "                np.random.shuffle(indices)\n",
    "                it = 0\n",
    "                for sid in indices:\n",
    "                    it += 1\n",
    "                    x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, _, _, _ = get_inputs(dbtrain, sid)\n",
    "                    targets = get_outputs(dbtrain, sid, n_targets)\n",
    "                    y_ck = np.squeeze(targets, axis=1)\n",
    "#                     x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(train_sentences[sid])\n",
    "#                     y_ck = get_output(train_sentences[sid])\n",
    "                    L = t_train(sess, x_word, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, y_ck)\n",
    "                print(\"Epoca \", (epoch+1))\n",
    "#                 dev_f1 = evaluate_conll(sess, df_ck, dev_sentences,t_inputs,t_pred,verbose=False)\n",
    "#                 gold_tags, eval_tags = tag_to_conll(sess, dbvalid, sample, idx2lex)\n",
    "#                 f1 = evaluate(gold_tags, eval_tags, verbose=True)\n",
    "                f1_valid = evaluate_dataset(sess, dbvalid, idx2lex, ds_type='valid')            \n",
    "\n",
    "                print(\"valid f1: \", f1_valid)\n",
    "#                 train_f1 = evaluate_conll(sess, df_ck, train_sentences,t_inputs,t_pred,verbose=False)\n",
    "                f1_train = evaluate_dataset(sess, dbtrain, idx2lex, ds_type='train')            \n",
    "                print(\"train f1: \", f1_train)\n",
    "                dev_f1_list.append(f1_valid)\n",
    "                train_f1_list.append(f1_train)\n",
    "                end = time.time()\n",
    "                print('Tempo por Epoca: ', (end-start), ' s')\n",
    "                if f1_valid > best_f1:\n",
    "                    best_f1 = f1_valid\n",
    "                    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                    save_path = save_dir + model_name + '_' + timestamp\n",
    "                    saver.save(sess, save_path)\n",
    "                    print('## BEST MODEL saved at ', save_path)\n",
    "                    consecutive_bad_dev = 0\n",
    "                else:\n",
    "                    consecutive_bad_dev += 1\n",
    "                if consecutive_bad_dev == 5:\n",
    "                    break\n",
    "            timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            exp_path = exp_desc_dir + model_name + timestamp + '.txt'\n",
    "            with open(exp_path,'w') as f:\n",
    "                exp_desc = {\n",
    "                    'model':model_meta,\n",
    "                    'f1_train':train_f1_list,\n",
    "                    'f1_valid':dev_f1_list\n",
    "                }\n",
    "                json.dump(exp_desc, f)\n",
    "                print('save results on ' + exp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
