{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/.venv/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/Varela/.venv/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../models/')\n",
    "sys.path.insert(0,'../datasets/')\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from utils.info import get_db_bounds\n",
    "from datasets import propbankbr_arg2se\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tqdm\n",
    "from models import PropbankEncoder\n",
    "import config \n",
    "\n",
    "INPUT_DIR = '../datasets/binaries/1.0/'\n",
    "PROPBANK_WAN50_PATH = '{:}wan50/deep_wan50.pickle'.format(INPUT_DIR)\n",
    "PEARL_SRLEVAL_PATH = '../srlconll04/srl-eval.pl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPN Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma mente insana realiza um experimento *lúcido* com o dataset de chunking da conll e seu script de avaliação :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>P</th>\n",
       "      <th>FORM</th>\n",
       "      <th>GPOS</th>\n",
       "      <th>MARKER</th>\n",
       "      <th>ARG</th>\n",
       "      <th>T</th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>CHUNK_START</th>\n",
       "      <th>CHUNK_FINISH</th>\n",
       "      <th>CHUNK_LEN</th>\n",
       "      <th>CHUNK_CANDIDATE_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDEX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Brasília</td>\n",
       "      <td>PROP</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pesquisa_Datafolha</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>(A0*</td>\n",
       "      <td>A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>publicada</td>\n",
       "      <td>V-PCP</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>hoje</td>\n",
       "      <td>ADV</td>\n",
       "      <td>0</td>\n",
       "      <td>*)</td>\n",
       "      <td>A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>revela</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>1</td>\n",
       "      <td>(V*)</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>um</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>(A1*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>dado</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>supreendente</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>recusando</td>\n",
       "      <td>V-GER</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>uma</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>postura</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>radical</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>esmagadora</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>maioria</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>(</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>%</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>os</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>eleitores</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>quer</td>\n",
       "      <td>V-FIN</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>PT</td>\n",
       "      <td>PROP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>participando</td>\n",
       "      <td>V-GER</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>ART</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Governo</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>Fernando_Henrique_Cardoso</td>\n",
       "      <td>PROP</td>\n",
       "      <td>1</td>\n",
       "      <td>*)</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PU</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  P                       FORM   GPOS  MARKER   ARG   T  CHUNK_ID  \\\n",
       "INDEX                                                                        \n",
       "0       1  1                   Brasília   PROP       0     *   *         1   \n",
       "1       2  1         Pesquisa_Datafolha      N       0  (A0*  A0         2   \n",
       "2       3  1                  publicada  V-PCP       0     *  A0         2   \n",
       "3       4  1                       hoje    ADV       0    *)  A0         2   \n",
       "4       5  1                     revela  V-FIN       1  (V*)   V         3   \n",
       "5       6  1                         um    ART       1  (A1*  A1         4   \n",
       "6       7  1                       dado      N       1     *  A1         4   \n",
       "7       8  1               supreendente    ADJ       1     *  A1         4   \n",
       "8       9  1                          :     PU       1     *  A1         4   \n",
       "9      10  1                  recusando  V-GER       1     *  A1         4   \n",
       "10     11  1                        uma    ART       1     *  A1         4   \n",
       "11     12  1                    postura      N       1     *  A1         4   \n",
       "12     13  1                    radical    ADJ       1     *  A1         4   \n",
       "13     14  1                          ,     PU       1     *  A1         4   \n",
       "14     15  1                          a    ART       1     *  A1         4   \n",
       "15     16  1                 esmagadora    ADJ       1     *  A1         4   \n",
       "16     17  1                    maioria      N       1     *  A1         4   \n",
       "17     18  1                          (     PU       1     *  A1         4   \n",
       "18     19  1                         77    NUM       1     *  A1         4   \n",
       "19     20  1                          %      N       1     *  A1         4   \n",
       "20     21  1                          )     PU       1     *  A1         4   \n",
       "21     22  1                         de    PRP       1     *  A1         4   \n",
       "22     23  1                         os    ART       1     *  A1         4   \n",
       "23     24  1                  eleitores      N       1     *  A1         4   \n",
       "24     25  1                       quer  V-FIN       1     *  A1         4   \n",
       "25     26  1                          o    ART       1     *  A1         4   \n",
       "26     27  1                         PT   PROP       1     *  A1         4   \n",
       "27     28  1               participando  V-GER       1     *  A1         4   \n",
       "28     29  1                         de    PRP       1     *  A1         4   \n",
       "29     30  1                          o    ART       1     *  A1         4   \n",
       "30     31  1                    Governo      N       1     *  A1         4   \n",
       "31     32  1  Fernando_Henrique_Cardoso   PROP       1    *)  A1         4   \n",
       "32     33  1                          .     PU       1     *   *         5   \n",
       "\n",
       "       CHUNK_START  CHUNK_FINISH  CHUNK_LEN  CHUNK_CANDIDATE_ID  \n",
       "INDEX                                                            \n",
       "0                0             1          1                   0  \n",
       "1                1             4          3                  35  \n",
       "2                1             4          3                  35  \n",
       "3                1             4          3                  35  \n",
       "4                4             5          1                 126  \n",
       "5                5            32         27                 181  \n",
       "6                5            32         27                 181  \n",
       "7                5            32         27                 181  \n",
       "8                5            32         27                 181  \n",
       "9                5            32         27                 181  \n",
       "10               5            32         27                 181  \n",
       "11               5            32         27                 181  \n",
       "12               5            32         27                 181  \n",
       "13               5            32         27                 181  \n",
       "14               5            32         27                 181  \n",
       "15               5            32         27                 181  \n",
       "16               5            32         27                 181  \n",
       "17               5            32         27                 181  \n",
       "18               5            32         27                 181  \n",
       "19               5            32         27                 181  \n",
       "20               5            32         27                 181  \n",
       "21               5            32         27                 181  \n",
       "22               5            32         27                 181  \n",
       "23               5            32         27                 181  \n",
       "24               5            32         27                 181  \n",
       "25               5            32         27                 181  \n",
       "26               5            32         27                 181  \n",
       "27               5            32         27                 181  \n",
       "28               5            32         27                 181  \n",
       "29               5            32         27                 181  \n",
       "30               5            32         27                 181  \n",
       "31               5            32         27                 181  \n",
       "32              32            33          1                 560  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfgs = pd.read_csv('../datasets/csvs/1.0/gs.csv', index_col=0, sep=',', encoding='utf-8')\n",
    "column_files = [\n",
    "    '../datasets/csvs/1.0/column_chunks/chunks.csv',\n",
    "    '../datasets/csvs/1.0/column_predmarker/predicate_marker.csv',\n",
    "    '../datasets/csvs/1.0/column_shifts_ctx_p/form.csv',\n",
    "    '../datasets/csvs/1.0/column_shifts_ctx_p/gpos.csv',\n",
    "    '../datasets/csvs/1.0/column_shifts_ctx_p/lemma.csv',\n",
    "    '../datasets/csvs/1.0/column_iob/iob.csv',\n",
    "    '../datasets/csvs/1.0/column_t/t.csv'\n",
    "]\n",
    "\n",
    "for col_f in column_files:\n",
    "    _df = pd.read_csv(col_f, index_col=0, encoding='utf-8')\n",
    "    dfgs = pd.concat((dfgs, _df), axis=1)\n",
    "\n",
    "DISPLAY_COLUMNS = ['ID', 'P', 'FORM', 'GPOS', 'MARKER', 'ARG', 'T', \n",
    "                   'CHUNK_ID', 'CHUNK_START', 'CHUNK_FINISH', 'CHUNK_LEN', 'CHUNK_CANDIDATE_ID']            \n",
    "dfgs[DISPLAY_COLUMNS].head(33)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Encodings\n",
    "\n",
    "Propbank Encoder holds an indexed version of propbank dataset an answers to FOUR different dataformats: \n",
    "* CAT: this is the raw categorical data.\n",
    "* EMB: tokens are embedding using GloVe embeddings.\n",
    "* HOT: onehot encoding of the words and tokens.\n",
    "* IDX: dense indexed representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ENCODER\n",
    "propbank_encoder = PropbankEncoder.recover(PROPBANK_WAN50_PATH)\n",
    "db = propbank_encoder.db\n",
    "lex2idx = propbank_encoder.lex2idx\n",
    "idx2lex = propbank_encoder.idx2lex\n",
    "\n",
    "# FOR TEXTUAL DATA ONLY\n",
    "tok2idx = propbank_encoder.tok2idx\n",
    "lex2tok = propbank_encoder.lex2tok\n",
    "idx2word = propbank_encoder.idx2word\n",
    "embeddings = propbank_encoder.embeddings\n",
    "\n",
    "n_targets = len(lex2idx['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes\t 43 \n",
      " records\t 138378\n"
     ]
    }
   ],
   "source": [
    "print('attributes\\t',\n",
    "       len(db),\n",
    "      '\\n',             \n",
    "      'records\\t',\n",
    "       len(db['ARG'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentences = df_ck.groupby('dataset')['sentence_id'].unique()\n",
    "train_sentences = data_sentences['train']\n",
    "dev_sentences = data_sentences['dev']\n",
    "test_sentences = data_sentences['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar informações das sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_str = df_ck.groupby('sentence_id')['word'].apply(list)\n",
    "sentence_words = df_ck.groupby('sentence_id')['word_norm-id'].apply(list)\n",
    "sentence_capts = df_ck.groupby('sentence_id')['capt-id'].apply(list)\n",
    "sentence_pos = df_ck.groupby('sentence_id')['cpos-id'].apply(list)\n",
    "sentence_ck = df_ck.groupby('sentence_id')['chunk-id'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de geração da entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(sentence_id):\n",
    "    a_txt = sentence_str[sentence_id]\n",
    "    a_words = list(sentence_words[sentence_id])\n",
    "    a_capt = list(sentence_capts[sentence_id])\n",
    "    a_pos = list(sentence_pos[sentence_id])\n",
    "    \n",
    "    a_words += [word_to_id['#END_SENT']]\n",
    "    a_capt += [capt_to_id['#END_SENT']]\n",
    "    a_pos += [pos_to_id['#END_SENT']]\n",
    "    \n",
    "    a_words = np.array(a_words)\n",
    "    a_capt = np.array(a_capt)\n",
    "    a_pos = np.array(a_pos)\n",
    "    \n",
    "    a_words = a_words.reshape((-1,1))\n",
    "    a_capt = a_capt.reshape((-1,1))\n",
    "    a_pos = a_pos.reshape((-1,1))\n",
    "    \n",
    "    a_chars = [[char_to_id[x] if x in char_to_id else char_to_id['#UNK'] for x in token] for token in a_txt] \n",
    "    \n",
    "    a_chars += [[char_to_id['#END_SENT']]]\n",
    "    \n",
    "    token_lens = np.array([len(x) for x in a_chars]).astype(np.int32) - 1\n",
    "    \n",
    "    n = len(token_lens)\n",
    "    m = np.max(np.array(token_lens)+1)\n",
    "    \n",
    "    char_matrix = np.ones((n,m))*len(char_to_id)\n",
    "    for i in range(n):\n",
    "        char_matrix[i][:token_lens[i]+1] = a_chars[i]\n",
    "        \n",
    "    return a_words, a_capt, a_chars, a_pos, char_matrix, token_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de geração da saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(sentence_id):\n",
    "    a_ck = list(sentence_ck[sentence_id])\n",
    "    a_ck.append(ck_to_id['#END_SENT'])\n",
    "    a_ck = np.array(a_ck)\n",
    "    return a_ck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_path(t_out, t_edge_scores, n_tags):\n",
    "    \n",
    "    def step(prev, et):\n",
    "        # last computed scores and last computed transitions\n",
    "        prev_scores, prev_selections = prev\n",
    "        \n",
    "        current_scores = tf.transpose(prev_scores + et)\n",
    "\n",
    "        best_scores = tf.reduce_max(current_scores, axis=0)\n",
    "        best_options = tf.argmax(current_scores, axis=0)\n",
    "\n",
    "        return best_scores, best_options\n",
    "    \n",
    "    score_matrix, selection_matrix = tf.scan(\n",
    "        fn=step,\n",
    "        elems=t_edge_scores,\n",
    "        initializer=(tf.zeros(n_tags), tf.to_int64(tf.zeros(n_tags))),\n",
    "    )\n",
    "    \n",
    "    return score_matrix, selection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_path(selection_matrix, last_tag):\n",
    "    \n",
    "    def step(prev, t):\n",
    "        selection_matrix, prev_best = prev\n",
    "\n",
    "        current = selection_matrix[t][prev_best]\n",
    "\n",
    "        return selection_matrix, current\n",
    "\n",
    "    m = tf.shape(selection_matrix)[0]\n",
    "    _, rev_path = tf.scan(\n",
    "        fn = step,\n",
    "        elems=m-1-tf.range(m),\n",
    "        initializer=(selection_matrix, last_tag)\n",
    "\n",
    "    )\n",
    "\n",
    "    best_path = tf.concat((tf.reverse(rev_path,axis=[0]),[last_tag]),axis=0)\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_score(t_edge_scores, path, n):\n",
    "    \n",
    "    def step(prev, t):\n",
    "        edge_scores, path, prev_score = prev\n",
    "        \n",
    "        transition_score = edge_scores[t]\n",
    "        \n",
    "        p_t = path[t]\n",
    "        p_tp1 = path[t+1]\n",
    "\n",
    "        current_score = transition_score[p_tp1,p_t] + prev_score\n",
    "\n",
    "        return edge_scores, path, current_score\n",
    "\n",
    "    _, _, path_score = tf.scan(\n",
    "        fn = step,\n",
    "        elems=tf.range(n-1),\n",
    "        initializer=(t_edge_scores, path, tf.zeros(1))\n",
    "\n",
    "    )\n",
    "    return path_score[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data/glove.6B.100d.txt','r') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer matriz de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17101\n"
     ]
    }
   ],
   "source": [
    "lines = content.split('\\n')\n",
    "lines = lines[:-1]\n",
    "\n",
    "word_embeds = {}\n",
    "\n",
    "for line in lines:\n",
    "    features = line.split()\n",
    "    word = features[0]\n",
    "    values = np.array(features[1:]).astype(np.float32)\n",
    "\n",
    "    word_embeds[word] = values\n",
    "\n",
    "embed_size = len(word_embeds['the'])\n",
    "embedding_matrix = np.zeros((len(id_to_word)+1,embed_size))\n",
    "\n",
    "found_words = 0\n",
    "for word in id_to_word:\n",
    "    if word in word_embeds:\n",
    "        found_words += 1\n",
    "        embedding_matrix[word_to_id[word]] = word_embeds[word]\n",
    "print(found_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-informação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    'vocab_size':len(id_to_word),\n",
    "    'capt_size':len(id_to_capt),\n",
    "    'pos_size':len(id_to_pos),\n",
    "    'char_size':len(id_to_char),\n",
    "    'ck_size':len(id_to_ck),\n",
    "    'embed_size':embedding_matrix.shape[1],\n",
    "    'capt_embed_size':10,\n",
    "    'char_embed_size':30,\n",
    "    'pos_embed_size':10,\n",
    "    'hidden_features':200,\n",
    "    'state_size':200,\n",
    "    'learning_rate':0.001,\n",
    "    'spn_layer':False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar modelo a partir da Meta-informacao e Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_meta, embedding_matrix):\n",
    "    vocab_size = model_meta['vocab_size']\n",
    "    capt_size = model_meta['capt_size']\n",
    "    pos_size = model_meta['pos_size']\n",
    "    char_size = model_meta['char_size']\n",
    "    ck_size = model_meta['ck_size']\n",
    "    embed_size = model_meta['embed_size']\n",
    "    capt_embed_size = model_meta['capt_embed_size']\n",
    "    char_embed_size = model_meta['char_embed_size']\n",
    "    char_hidden = model_meta['char_hidden_features']\n",
    "    pos_embed_size = model_meta['pos_embed_size']\n",
    "    hidden_features = model_meta['hidden_features']\n",
    "    state_size = model_meta['state_size']\n",
    "    lr = model_meta['learning_rate']\n",
    "    spn_layer = model_meta['spn_layer']\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    t_x_words = tf.placeholder(tf.int32,(None,1)) # ids das palavras\n",
    "    t_x_capt = tf.placeholder(tf.int32,(None,1)) # ids de capitalizacao\n",
    "    t_x_pos = tf.placeholder(tf.int32,(None,1)) # ids de POS\n",
    "    t_x_chars = tf.placeholder(tf.int32, (None,None))\n",
    "    t_x_lens = tf.placeholder(tf.int32, (None))\n",
    "    t_y_ck = tf.placeholder(tf.int32, shape=(None,)) # ids das classes de POS para cada token\n",
    "    \n",
    "    t_inputs = [t_x_words, t_x_capt, t_x_pos, t_x_chars, t_x_lens]\n",
    "    t_targets = [t_y_ck]\n",
    "\n",
    "    with tf.variable_scope('Feature_Vars'):\n",
    "        t_W_embed = tf.Variable(embedding_matrix.astype(np.float32))\n",
    "        t_W_char = tf.Variable(np.random.normal(0,0.1,(char_size+1, char_embed_size)).astype(np.float32))\n",
    "\n",
    "        t_gamma = tf.Variable(np.random.normal(0,1.0, 1).astype(np.float32))\n",
    "        t_beta = tf.Variable(np.random.normal(0,1.0, 1).astype(np.float32))\n",
    "\n",
    "        dim = state_size\n",
    "        if dim is None:\n",
    "            dim = 2*hidden_features\n",
    "        if spn_layer:\n",
    "            t_W_tran = tf.Variable(np.random.normal(0,1.0/np.sqrt(3*dim),(3*dim, ck_size*ck_size)).astype(np.float32))\n",
    "        else:\n",
    "            t_W_ck = tf.Variable(np.random.normal(0,0.1,(dim, ck_size)).astype(np.float32))\n",
    "        \n",
    "\n",
    "\n",
    "    t_words = tf.gather_nd(t_W_embed, t_x_words)\n",
    "    t_x_pos_flat = tf.squeeze(t_x_pos,axis=1)\n",
    "    t_x_capt_flat = tf.squeeze(t_x_capt,axis=1)\n",
    "\n",
    "    t_capt = tf.to_float(tf.one_hot(indices=t_x_capt_flat, depth=capt_size+1,on_value=1,off_value=0))\n",
    "    t_pos = tf.to_float(tf.one_hot(indices=t_x_pos_flat,depth=pos_size+1,on_value=1,off_value=0))\n",
    "\n",
    "    # t_capt = tf.gather_nd(t_W_capt, t_x_capt)\n",
    "    # t_pos = tf.gather_nd(t_W_pos, t_x_pos)\n",
    "    t_char = tf.nn.embedding_lookup(t_W_char, t_x_chars)\n",
    "\n",
    "    t_lstmcell_char = tf.nn.rnn_cell.LSTMCell(num_units=char_hidden, state_is_tuple=True)\n",
    "\n",
    "    with tf.variable_scope('LSTM_Chars'):\n",
    "        t_h, t_ls = tf.nn.dynamic_rnn(\n",
    "            cell=t_lstmcell_char,\n",
    "            dtype=tf.float32,\n",
    "            inputs=t_char,\n",
    "        )\n",
    "\n",
    "    t_x_ind = tf.range(tf.shape(t_x_lens)[0])\n",
    "    t_x_get = tf.transpose(tf.stack([t_x_ind, t_x_lens]))\n",
    "    t_x_token_char = tf.gather_nd(t_h, t_x_get)\n",
    "\n",
    "    t_word_feats = tf.concat((t_words, t_capt, t_pos, t_x_token_char), axis=1)\n",
    "\n",
    "    t_sq_len = tf.shape(t_x_words)[0]\n",
    "\n",
    "    n_features = embed_size\n",
    "\n",
    "    t_words_shp = tf.reshape(\n",
    "        t_word_feats, (1,t_sq_len, pos_size+1 +embed_size+capt_size+1+char_hidden))\n",
    "\n",
    "    t_lstmcellf = tf.nn.rnn_cell.LSTMCell(num_units=hidden_features, state_is_tuple=True)\n",
    "    t_lstmcellb = tf.nn.rnn_cell.LSTMCell(num_units=hidden_features, state_is_tuple=True)\n",
    "\n",
    "    with tf.variable_scope(\"Bilstm\"):\n",
    "\n",
    "        t_h1, t_last_states =tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=t_lstmcellf,\n",
    "            cell_bw=t_lstmcellb,\n",
    "            dtype=tf.float32,\n",
    "            inputs=t_words_shp)\n",
    "\n",
    "        t_hidden = tf.concat((t_h1[0][0],t_h1[1][0]),axis=1)\n",
    "\n",
    "    if state_size is not None:\n",
    "        t_lstmcell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "        t_hidden_shp = tf.reshape(t_hidden, (1,t_sq_len, 2*hidden_features))\n",
    "\n",
    "        with tf.variable_scope('LSTM_last'):\n",
    "            t_h2, t_last_states2 =tf.nn.dynamic_rnn(\n",
    "                cell=t_lstmcell,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=[t_sq_len],\n",
    "                inputs=t_hidden_shp)\n",
    "\n",
    "        t_out = t_h2[0]\n",
    "    else:\n",
    "        t_out = t_hidden\n",
    "        \n",
    "    n = tf.shape(t_out)[0]\n",
    "    \n",
    "    t_outputs = []\n",
    "    if spn_layer:\n",
    "        t_edges = tf.concat((t_out[1:],t_out[:-1],t_out[1:]*t_out[:-1]),axis=1)\n",
    "        t_edge_scores = tf.matmul(t_edges, t_W_tran)\n",
    "\n",
    "        t_edge_scores = tf.reshape(t_edge_scores, ((n-1)*ck_size*ck_size,))\n",
    "\n",
    "        # Batch Normalization\n",
    "        t_es_mean = tf.reduce_mean(t_edge_scores)\n",
    "        t_es_m2 = tf.reduce_mean(t_edge_scores**2)\n",
    "\n",
    "        t_es_var = t_es_m2 - t_es_mean**2\n",
    "        t_es_std = tf.sqrt(t_es_var + 1e-8)\n",
    "\n",
    "        t_es_norm = (t_edge_scores - t_es_mean)/t_es_std\n",
    "\n",
    "        t_es_renorm = t_gamma * t_es_norm + t_beta\n",
    "\n",
    "        t_edge_scores = tf.reshape(t_es_renorm, (n-1,ck_size,ck_size))\n",
    "\n",
    "        t_score_matrix, t_selection_matrix = longest_path(t_out, t_edge_scores,ck_size)\n",
    "\n",
    "        t_best_score = tf.reduce_max(t_score_matrix[-1])\n",
    "        t_last_tag = tf.argmax(t_score_matrix[-1])\n",
    "\n",
    "        t_best_path = retrieve_path(t_selection_matrix, t_last_tag)\n",
    "\n",
    "        t_correct_score = path_score(t_edge_scores, t_y_ck, n)\n",
    "\n",
    "        t_cost = -t_correct_score\n",
    "\n",
    "        # # gradiente descendente no custo do perceptron estruturado\n",
    "        t_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        t_train = t_optimizer.minimize(t_cost)\n",
    "\n",
    "        t_outputs.extend([t_score_matrix, t_best_path])\n",
    "\n",
    "        def t_pred(sess, inputs, x_words, x_capt, x_pos, x_char_matrix, x_char_lens):\n",
    "            result = sess.run(t_best_path, feed_dict={\n",
    "                inputs[0]:x_words,\n",
    "                inputs[1]:x_capt,\n",
    "                inputs[2]:x_pos,\n",
    "                inputs[3]:x_char_matrix,\n",
    "                inputs[4]:x_char_lens\n",
    "            })\n",
    "            return result\n",
    "\n",
    "        def my_t_train(sess, inputs,targets, x_words, x_capt, x_pos, x_char_matrix, x_char_lens, y_ck):\n",
    "            _, result = sess.run([t_train, t_cost], feed_dict={\n",
    "                inputs[0]:x_words,\n",
    "                inputs[1]:x_capt,\n",
    "                inputs[2]:x_pos,\n",
    "                inputs[3]:x_char_matrix,\n",
    "                inputs[4]:x_lens,\n",
    "                targets[0]:y_ck,\n",
    "            })\n",
    "            return result\n",
    "    else: #CRF\n",
    "        t_ck_score = tf.matmul(t_out,t_W_ck)\n",
    "\n",
    "        t_ck_score_ext = tf.expand_dims(t_ck_score, 0)\n",
    "        t_y_ck_ext = tf.expand_dims(t_y_ck, 0)\n",
    "\n",
    "        t_sequence_lengths = tf.shape(t_x_words)[0]\n",
    "        t_sequence_lengths = tf.expand_dims(t_sequence_lengths,0)\n",
    "\n",
    "        t_log_likelihood, t_transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            t_ck_score_ext, \n",
    "            t_y_ck_ext, \n",
    "            t_sequence_lengths)\n",
    "        \n",
    "        t_outputs.extend([t_ck_score, t_transition_params])\n",
    "    \n",
    "        t_cost = -t_log_likelihood\n",
    "\n",
    "        # # gradiente descendente no custo do perceptron estruturado\n",
    "        t_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(0.003)\n",
    "        t_train = t_optimizer.minimize(t_cost)\n",
    "\n",
    "        def t_pred(sess, inputs, x_words, x_capt, x_pos, x_char_matrix, x_char_lens):\n",
    "            score, tparams = sess.run([t_ck_score,t_transition_params], feed_dict={\n",
    "                inputs[0]:x_words,\n",
    "                inputs[1]:x_capt,\n",
    "                inputs[2]:x_pos,\n",
    "                inputs[3]:x_char_matrix,\n",
    "                inputs[4]:x_char_lens\n",
    "            })\n",
    "\n",
    "            return tf.contrib.crf.viterbi_decode(score=score,transition_params=tparams)[0][:-1]\n",
    "        \n",
    "        def my_t_train(sess, inputs,targets, x_words, x_capt, x_pos, x_char_matrix, x_char_lens, y_ck):\n",
    "            _, result = sess.run([t_train, t_cost], feed_dict={\n",
    "                inputs[0]:x_words,\n",
    "                inputs[1]:x_capt,\n",
    "                inputs[2]:x_pos,\n",
    "                inputs[3]:x_char_matrix,\n",
    "                inputs[4]:x_lens,\n",
    "                targets[0]:y_ck,\n",
    "            })\n",
    "            return result\n",
    "    \n",
    "    return t_inputs, t_targets, t_outputs, my_t_train, t_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/.local/share/virtualenvs/mop-YlMLCBLu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model_meta = {\n",
    "    'vocab_size':len(id_to_word),\n",
    "    'capt_size':len(id_to_capt),\n",
    "    'pos_size':len(id_to_pos),\n",
    "    'char_size':len(id_to_char),\n",
    "    'ck_size':len(id_to_ck),\n",
    "    'embed_size':embedding_matrix.shape[1],\n",
    "    'capt_embed_size':10,\n",
    "    'char_hidden_features':30,\n",
    "    'char_embed_size':30,\n",
    "    'pos_embed_size':10,\n",
    "    'hidden_features':200,\n",
    "    'state_size':200,\n",
    "    'learning_rate':0.001,\n",
    "    'spn_layer':False\n",
    "}\n",
    "\n",
    "t_inputs, t_targets, t_outputs, t_train, t_pred = make_model(model_meta, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 10, 17, 10, 17, 10, 17, 10, 17, 10, 17, 10, 17, 10, 17, 10, 17, 10, 17, 10]\n"
     ]
    }
   ],
   "source": [
    "sample = 968\n",
    "x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(sample)\n",
    "y_ck = get_output(sample)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    o_ck = t_pred(sess, t_inputs, x_words, x_capt, x_pos, x_char_matrix, x_lens)\n",
    "print(o_ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorar uma sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66.59537]\n",
      "[0.1083374]\n"
     ]
    }
   ],
   "source": [
    "sample =968\n",
    "x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(sample)\n",
    "y_ck = get_output(sample)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(200):\n",
    "    L = t_train(sess,t_inputs,t_targets, x_words,x_capt,x_pos,x_char_matrix,x_lens,y_ck)\n",
    "    if i % 100 == 0:\n",
    "        print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_conll(sess, df_chunk,sentences, t_inputs,t_pred):\n",
    "    result = ''\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        df_sentence = df_chunk[df_chunk['sentence_id']==sentence]\n",
    "        s_tags = np.array(df_sentence['chunk'])\n",
    "        s_words = np.array(df_sentence['word'])\n",
    "        \n",
    "        x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(sentence)\n",
    "        y_ck = get_output(sentence)[:-1]\n",
    "        o_ck = t_pred(sess, t_inputs, x_words, x_capt, x_pos, x_char_matrix, x_lens)[:-1]\n",
    "        \n",
    "        tags = [id_to_ck[x] for x in o_ck]\n",
    "        \n",
    "        for i in range(len(o_ck)):\n",
    "            word_str = s_words[i] + ' ' + s_tags[i] + ' ' + tags[i]\n",
    "            result += word_str + '\\n'\n",
    "        result += '\\n'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_conll(sess, df_chunk,sentences,t_inputs,t_pred,verbose=True):\n",
    "    result = tag_to_conll(sess, df_chunk, sentences,t_inputs,t_pred)\n",
    "    f1 = 0\n",
    "    path = 'conlleval.txt'\n",
    "    p = Popen(['perl', path], stdout=PIPE, stdin=PIPE, stderr=STDOUT)\n",
    "    conlleval_stdout = p.communicate(input=result.encode())[0]\n",
    "    if verbose:\n",
    "        print(conlleval_stdout.decode())\n",
    "    try:\n",
    "        f1 = float(re.findall(r'(\\d+\\.?\\d+)', conlleval_stdout.decode())[7])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(conlleval_stdout.decode())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino Parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_meta = {\n",
    "#     'vocab_size':len(id_to_word),\n",
    "#     'capt_size':len(id_to_capt),\n",
    "#     'pos_size':len(id_to_pos),\n",
    "#     'char_size':len(id_to_char),\n",
    "#     'ck_size':len(id_to_ck),\n",
    "#     'embed_size':embedding_matrix.shape[1],\n",
    "#     'capt_embed_size':10,\n",
    "#     'char_hidden_features':30,\n",
    "#     'char_embed_size':30,\n",
    "#     'pos_embed_size':10,\n",
    "#     'hidden_features':200,\n",
    "#     'state_size':200,\n",
    "#     'learning_rate':0.001,\n",
    "#     'spn_layer':True\n",
    "# }\n",
    "\n",
    "# t_inputs, t_targets, t_outputs, t_train, t_pred = make_model(model_meta, embedding_matrix)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# n_epochs = 3\n",
    "\n",
    "# indices = np.arange(len(train_sentences)//100)    \n",
    "# n = len(indices)\n",
    "\n",
    "# best_path = None\n",
    "# best_f1 = 0\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     j = 0\n",
    "#     np.random.shuffle(indices)\n",
    "#     for sid in indices:\n",
    "#         j += 1\n",
    "#         x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(train_sentences[sid])\n",
    "#         y_ck = get_output(train_sentences[sid])\n",
    "#         L = t_train(sess,t_inputs,t_targets,x_words,x_capt,x_pos,x_char_matrix,x_lens,y_ck)\n",
    "#         if j % (n//10) == 0:\n",
    "#             print('{:.2f} %'.format(100*j/n))\n",
    "        \n",
    "#     print(\"Epoca \", (epoch+1))\n",
    "#     dev_f1 = evaluate_conll(sess, df_ck, dev_sentences[:len(indices)],t_inputs,t_pred,verbose=False)\n",
    "#     print(\"dev f1: \", dev_f1)\n",
    "#     train_f1 = evaluate_conll(sess, df_ck, train_sentences[:len(indices)],t_inputs,t_pred,verbose=False)\n",
    "#     print(\"train f1: \", train_f1)\n",
    "    \n",
    "#     if dev_f1 > best_f1:\n",
    "#         best_f1 = dev_f1\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   3\n",
      "Training Model {'vocab_size': 18687, 'capt_size': 9, 'pos_size': 45, 'char_size': 85, 'ck_size': 24, 'embed_size': 100, 'capt_embed_size': 10, 'char_embed_size': 30, 'pos_embed_size': 10, 'char_hidden_features': 30, 'hidden_features': 50, 'state_size': 50, 'learning_rate': 0.0001, 'spn_layer': True}\n",
      "Epoca  1\n",
      "dev f1:  78.3\n",
      "train f1:  76.87\n",
      "Tempo por Epoca:  547.1610581874847  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_11-13-17\n",
      "Epoca  2\n",
      "dev f1:  83.84\n",
      "train f1:  82.68\n",
      "Tempo por Epoca:  510.7972779273987  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_11-21-48\n",
      "Epoca  3\n",
      "dev f1:  85.89\n",
      "train f1:  84.73\n",
      "Tempo por Epoca:  552.3075170516968  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_11-31-01\n",
      "Epoca  4\n",
      "dev f1:  88.4\n",
      "train f1:  87.26\n",
      "Tempo por Epoca:  496.2450067996979  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_11-39-17\n",
      "Epoca  5\n",
      "dev f1:  89.8\n",
      "train f1:  89.0\n",
      "Tempo por Epoca:  486.29511404037476  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_11-47-24\n",
      "Epoca  6\n",
      "dev f1:  91.11\n",
      "train f1:  90.53\n",
      "Tempo por Epoca:  492.8893520832062  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_11-55-38\n",
      "Epoca  7\n",
      "dev f1:  91.53\n",
      "train f1:  91.51\n",
      "Tempo por Epoca:  495.5707359313965  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-03-54\n",
      "Epoca  8\n",
      "dev f1:  92.11\n",
      "train f1:  92.16\n",
      "Tempo por Epoca:  468.1980302333832  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-11-42\n",
      "Epoca  9\n",
      "dev f1:  92.52\n",
      "train f1:  92.88\n",
      "Tempo por Epoca:  459.84623098373413  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-19-23\n",
      "Epoca  10\n",
      "dev f1:  92.93\n",
      "train f1:  93.45\n",
      "Tempo por Epoca:  462.2496147155762  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-27-05\n",
      "Epoca  11\n",
      "dev f1:  93.06\n",
      "train f1:  93.92\n",
      "Tempo por Epoca:  468.68000316619873  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-34-55\n",
      "Epoca  12\n",
      "dev f1:  93.22\n",
      "train f1:  94.39\n",
      "Tempo por Epoca:  473.60065507888794  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-42-49\n",
      "Epoca  13\n",
      "dev f1:  93.5\n",
      "train f1:  94.91\n",
      "Tempo por Epoca:  542.5684700012207  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_12-51-52\n",
      "Epoca  14\n",
      "dev f1:  93.81\n",
      "train f1:  95.23\n",
      "Tempo por Epoca:  545.6424179077148  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_13-00-58\n",
      "Epoca  15\n",
      "dev f1:  93.92\n",
      "train f1:  95.65\n",
      "Tempo por Epoca:  474.532986164093  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_13-08-54\n",
      "Epoca  16\n",
      "dev f1:  93.96\n",
      "train f1:  95.95\n",
      "Tempo por Epoca:  463.54125118255615  s\n",
      "## BEST MODEL saved at  ../../../data/CHUNK/en/models/bilstm_viterbi_h50_lr_0.0001_spn_2018-10-09_13-16-38\n",
      "Epoca  17\n",
      "dev f1:  93.93\n",
      "train f1:  96.06\n",
      "Tempo por Epoca:  451.6280927658081  s\n",
      "Epoca  18\n",
      "dev f1:  93.95\n",
      "train f1:  96.33\n",
      "Tempo por Epoca:  2290.50719499588  s\n"
     ]
    }
   ],
   "source": [
    "lr_search = [0.0001]\n",
    "hidden_features_search = [50,150,250]\n",
    "\n",
    "for j in range(len(hidden_features_search)):\n",
    "    for i in range(len(lr_search)):\n",
    "        for s in range(2):\n",
    "            lr = lr_search[i]\n",
    "            hf = hidden_features_search[j]\n",
    "\n",
    "            print(j, ' ', len(hidden_features_search))\n",
    "            model_meta = {\n",
    "                'vocab_size':len(id_to_word),\n",
    "                'capt_size':len(id_to_capt),\n",
    "                'pos_size':len(id_to_pos),\n",
    "                'char_size':len(id_to_char),\n",
    "                'ck_size':len(id_to_ck),\n",
    "                'embed_size':embedding_matrix.shape[1],\n",
    "                'capt_embed_size':10,\n",
    "                'char_embed_size':30,\n",
    "                'pos_embed_size':10,\n",
    "                'char_hidden_features':30,\n",
    "                'hidden_features':hf,\n",
    "                'state_size':hf,\n",
    "                'learning_rate':lr,\n",
    "                'spn_layer':s==0\n",
    "            }\n",
    "            print('Training Model', model_meta)\n",
    "            \n",
    "            last_layer = 'crf'\n",
    "            if s == 0:\n",
    "                last_layer = 'spn'\n",
    "\n",
    "            n_epochs = 30\n",
    "            model_name = 'bilstm_viterbi_h' + str(hf) + '_lr_' + str(lr) + '_' + last_layer\n",
    "            model = make_model(model_meta, embedding_matrix)\n",
    "\n",
    "\n",
    "            t_inputs, t_targets, t_outputs, t_train, t_pred = model\n",
    "            \n",
    "            #NEW SESSION PER MODEL\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            sess = tf.Session(config=config)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            indices = np.arange(len(train_sentences))\n",
    "            n = len(indices)\n",
    "\n",
    "            best_path = None\n",
    "            best_f1 = 0\n",
    "\n",
    "            save_dir = '../../../data/CHUNK/' + language + '/models/'\n",
    "            exp_desc_dir = 'results/'\n",
    "            saver = tf.train.Saver(max_to_keep=n_epochs*20)\n",
    "\n",
    "            dev_f1_list = []\n",
    "            train_f1_list = []\n",
    "            consecutive_bad_dev = 0\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                start = time.time()\n",
    "                np.random.shuffle(indices)\n",
    "                it = 0\n",
    "                for sid in indices:\n",
    "                    it += 1\n",
    "                    x_words, x_capt, x_chars, x_pos, x_char_matrix, x_lens = get_input(train_sentences[sid])\n",
    "                    y_ck = get_output(train_sentences[sid])\n",
    "                    L = t_train(sess,t_inputs, t_targets, x_words,x_capt,x_pos,x_char_matrix,x_lens,y_ck)\n",
    "                print(\"Epoca \", (epoch+1))\n",
    "                dev_f1 = evaluate_conll(sess, df_ck, dev_sentences,t_inputs,t_pred,verbose=False)\n",
    "                print(\"dev f1: \", dev_f1)\n",
    "                train_f1 = evaluate_conll(sess, df_ck, train_sentences,t_inputs,t_pred,verbose=False)\n",
    "                print(\"train f1: \", train_f1)\n",
    "                dev_f1_list.append(dev_f1)\n",
    "                train_f1_list.append(train_f1)\n",
    "                end = time.time()\n",
    "                print('Tempo por Epoca: ', (end-start), ' s')\n",
    "                if dev_f1 > best_f1:\n",
    "                    best_f1 = dev_f1\n",
    "                    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                    save_path = save_dir + model_name + '_' + timestamp\n",
    "                    saver.save(sess, save_path)\n",
    "                    print('## BEST MODEL saved at ', save_path)\n",
    "                    consecutive_bad_dev = 0\n",
    "                else:\n",
    "                    consecutive_bad_dev += 1\n",
    "                if consecutive_bad_dev == 5:\n",
    "                    break\n",
    "            timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            exp_path = exp_desc_dir + model_name + timestamp + '.txt'\n",
    "            with open(exp_path,'w') as f:\n",
    "                exp_desc = {\n",
    "                    'model':model_meta,\n",
    "                    'train_f1':train_f1_list,\n",
    "                    'dev_f1':dev_f1_list\n",
    "                }\n",
    "                json.dump(exp_desc, f)\n",
    "                print('save results on ' + exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
